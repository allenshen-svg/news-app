#!/usr/bin/env python3
"""
æ¨¡å—å››ï¼šç³»ç»Ÿæ•´ä½“æ¶æ„ - çƒ­ç‚¹å‘ç°ç¼–æ’å™¨
============================================================

ç³»ç»Ÿæ¶æ„ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     å®šæ—¶è°ƒåº¦å±‚ (Cron / Loop)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  æŠ–éŸ³     â”‚  â”‚  å°çº¢ä¹¦   â”‚  â”‚  å¾®åš     â”‚  â”‚ Bç«™/çŸ¥ä¹  â”‚  â”‚
â”‚  â”‚ Crawler  â”‚  â”‚ Crawler  â”‚  â”‚ Crawler  â”‚  â”‚ Crawler  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â”‚       â”‚             â”‚             â”‚             â”‚          â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜             â”‚          â”‚
â”‚              â–¼             â”‚                    â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚                    â”‚          â”‚
â”‚  â”‚  Raw Data Store   â”‚â—„â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚  â”‚  data/raw_feeds/  â”‚                                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚           â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚  NLP Pipeline     â”‚  jiebaåˆ†è¯ â†’ TF-IDF â†’ TextRank     â”‚
â”‚  â”‚  å…³é”®è¯æå–        â”‚  å®ä½“è¯†åˆ« â†’ æ–°è¯å‘ç°                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚           â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚  Time Series DB   â”‚  keyword_history.json               â”‚
â”‚  â”‚  æ»‘åŠ¨çª—å£ç»Ÿè®¡      â”‚  10minçª—å£ Ã— 144 = 24h             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚           â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚  Burst Detector   â”‚  Z-Score + MACD + Newton Cooling    â”‚
â”‚  â”‚  çªå‘æ£€æµ‹          â”‚                                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚           â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚  Heat Scorer      â”‚  H(t) = Î±FÂ·e^{-Î»Î”t} + Î²A + Î³S + Î´Eâ”‚
â”‚  â”‚  çƒ­åŠ›è®¡ç®—          â”‚                                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚           â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  trends.json      â”‚â”€â”€â”€â”€â–¶â”‚  å‰ç«¯å±•ç¤ºå±‚        â”‚           â”‚
â”‚  â”‚  (è¶‹åŠ¿ç»“æœ)        â”‚     â”‚  index.html        â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ ¸å¿ƒæŠ€æœ¯æ ˆï¼š                                               â”‚
â”‚  â€¢ Python 3.9+ (é‡‡é›†+å¤„ç†)                                  â”‚
â”‚  â€¢ jieba (ä¸­æ–‡åˆ†è¯/TF-IDF/TextRank)                         â”‚
â”‚  â€¢ requests + Session (HTTPé‡‡é›†)                            â”‚
â”‚  â€¢ JSONæ–‡ä»¶ (è½»é‡å­˜å‚¨ï¼Œå¯æ›¿æ¢ä¸ºRedis/MongoDB)                â”‚
â”‚  â€¢ å‰ç«¯: Vanilla JS + SVG Sparklines (å¯è§†åŒ–)               â”‚
â”‚                                                            â”‚
â”‚  ç”Ÿäº§çº§æ‰©å±•æ–¹å‘ï¼š                                            â”‚
â”‚  â€¢ é‡‡é›†å±‚ â†’ Scrapy/Playwright é›†ç¾¤                          â”‚
â”‚  â€¢ æ¶ˆæ¯é˜Ÿåˆ— â†’ Kafka/RabbitMQ                                â”‚
â”‚  â€¢ æµå¤„ç† â†’ Flink/Spark Streaming                           â”‚
â”‚  â€¢ å­˜å‚¨ â†’ Redis(çƒ­æ•°æ®) + MongoDB(å†·æ•°æ®)                    â”‚
â”‚  â€¢ NLP â†’ HanLP/LAC + BERT-NER                              â”‚
â”‚  â€¢ éƒ¨ç½² â†’ Docker + K8s                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä½¿ç”¨æ–¹å¼ï¼š
  # å•æ¬¡è¿è¡Œ
  python3 scripts/discover_trends.py
  
  # å¾ªç¯è¿è¡Œ (æ¯10åˆ†é’Ÿ)
  python3 scripts/discover_trends.py --loop 10
  
  # æŒ‡å®šå¹³å°
  python3 scripts/discover_trends.py --platforms weibo,bilibili,zhihu
  
  # åŒæ—¶æ›´æ–°æ–°é—»
  python3 scripts/discover_trends.py --with-news
"""

import os
import sys
import json
import time
import logging
import traceback
from datetime import datetime, timezone, timedelta
from pathlib import Path

# ç¡®ä¿å¯ä»¥å¯¼å…¥åŒç›®å½•æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))

from feed_crawler import CrawlOrchestrator, RawContent
from trend_engine import TrendEngine

# ==================== é…ç½® ====================
DATA_DIR = Path(__file__).parent.parent / "data"
TRENDS_FILE = DATA_DIR / "trends.json"
NEWS_FILE = DATA_DIR / "news.json"

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger('discover')


def load_news_as_raw(news_file=None) -> list:
    """
    å°† news.json ä¸­çš„å·²æŠ“å–æ–°é—»è½¬æ¢ä¸º RawContent æ ¼å¼
    
    ç”¨é€”ï¼šå³ä½¿çˆ¬è™«å…¨éƒ¨å¤±è´¥ï¼Œä¹Ÿèƒ½åˆ©ç”¨å·²æœ‰æ–°é—»æ•°æ®åšè¶‹åŠ¿åˆ†æ
    """
    if news_file is None:
        news_file = NEWS_FILE
    
    items = []
    try:
        if not Path(news_file).exists():
            return items
        
        with open(news_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        news_list = data.get('items', [])
        for n in news_list:
            # è·³è¿‡ä¹‹å‰å‘ç°çš„è¶‹åŠ¿ï¼ˆé¿å…å¾ªç¯å¼•ç”¨ï¼‰
            if n.get('is_discovered_trend'):
                continue
            
            title = n.get('title', '').strip()
            if not title or len(title) < 4:
                continue
            
            source = n.get('source', '')
            # æ˜ å°„åˆ°å¹³å°å
            platform_map = {
                'æŠ–éŸ³çƒ­æœ': 'douyin',
                'å°çº¢ä¹¦çƒ­é—¨': 'xiaohongshu', 
                'ä»Šæ—¥å¤´æ¡': 'toutiao',
                '36æ°ªå¿«è®¯': '36kr',
                'æ–°æµªè´¢ç»': 'sina',
            }
            platform = 'news'
            for key, val in platform_map.items():
                if key in source:
                    platform = val
                    break
            
            items.append(RawContent(
                platform=platform,
                content_id=f"news_{n.get('id', '')}",
                title=title,
                text=n.get('summary', title),
                views=n.get('hot_value', 0) or 0,
                tags=[n.get('category', '')],
                url=n.get('link', ''),
                pub_time=n.get('pub_date', ''),
                crawl_time=n.get('fetch_time', ''),
                content_type='article',
                extra={'source': source, 'lang': n.get('lang', '')}
            ))
        
        logger.info(f"  ğŸ“° ä» news.json åŠ è½½ {len(items)} æ¡æ–°é—»ä½œä¸ºè¡¥å……æ•°æ®")
    except Exception as e:
        logger.error(f"  âš ï¸ åŠ è½½ news.json å¤±è´¥: {str(e)[:60]}")
    
    return items


def discover_trends(platforms=None, keyword_count=10, topK=50, proxy=None):
    """
    æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„çƒ­ç‚¹å‘ç°æµç¨‹
    
    1. å¤šå¹³å°æ•°æ®é‡‡é›†
    2. è¡¥å……å·²æœ‰æ–°é—»æ•°æ® (news.json)
    3. NLP å…³é”®è¯æå–
    4. çªå‘æ£€æµ‹
    5. çƒ­åŠ›å€¼è®¡ç®—
    6. è¾“å‡ºè¶‹åŠ¿æ’å
    """
    start_time = time.time()
    
    print(f"\n{'='*60}")
    print(f"ğŸ”¬ çƒ­ç‚¹å‘ç°ç³»ç»Ÿ v2.0")
    print(f"   æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"   å¹³å°: {', '.join(platforms) if platforms else 'å…¨éƒ¨'}")
    print(f"{'='*60}")
    
    # â”€â”€ é˜¶æ®µ1: æ•°æ®é‡‡é›† â”€â”€
    orchestrator = CrawlOrchestrator(proxy=proxy)
    raw_items = orchestrator.crawl_all(
        platforms=platforms,
        keyword_count=keyword_count
    )
    
    # â”€â”€ é˜¶æ®µ1.5: è¡¥å……å·²æœ‰æ–°é—»æ•°æ® â”€â”€
    news_items = load_news_as_raw()
    if news_items:
        raw_items.extend(news_items)
        print(f"  ğŸ“Š çˆ¬è™« {len(raw_items) - len(news_items)} æ¡ + æ–°é—» {len(news_items)} æ¡ = æ€»è®¡ {len(raw_items)} æ¡")
    
    if not raw_items:
        print("\nâš ï¸ æœªé‡‡é›†åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ python3 scripts/fetch_news.py")
        return []
    
    # â”€â”€ é˜¶æ®µ2: è¶‹åŠ¿åˆ†æ â”€â”€
    engine = TrendEngine()
    trends = engine.process(raw_items, topK=topK)
    
    # â”€â”€ é˜¶æ®µ3: ä¿å­˜ç»“æœ â”€â”€
    engine.save_trends(trends, TRENDS_FILE)
    
    # â”€â”€ é˜¶æ®µ4: åˆå¹¶åˆ°æ–°é—»æ•°æ® â”€â”€
    merge_trends_to_news(trends)
    
    elapsed = time.time() - start_time
    
    # è¾“å‡ºæ‘˜è¦
    print(f"\n{'='*60}")
    print(f"âœ… çƒ­ç‚¹å‘ç°å®Œæˆ ({elapsed:.1f}s)")
    print(f"   é‡‡é›†å†…å®¹: {len(raw_items)} æ¡")
    print(f"   å‘ç°è¶‹åŠ¿: {len(trends)} ä¸ª")
    print(f"   çªå‘çƒ­ç‚¹: {sum(1 for t in trends if t.is_burst)} ä¸ª")
    print(f"   ä¸Šå‡è¶‹åŠ¿: {sum(1 for t in trends if t.trend_direction in ('â†‘','â†—'))} ä¸ª")
    print(f"{'='*60}")
    
    if trends:
        print(f"\nğŸ† Top 10 çƒ­ç‚¹:")
        for i, t in enumerate(trends[:10]):
            burst = 'ğŸ”´ BURST' if t.is_burst else ''
            direction = t.trend_direction
            platforms = ','.join(t.platforms[:3])
            print(f"   {i+1:2d}. [{t.heat_score:5.1f}] {direction} {t.keyword:12s} "
                  f"| freq={t.frequency:3d} | {platforms:20s} {burst}")
    
    return trends


def merge_trends_to_news(trends):
    """
    å°†å‘ç°çš„è¶‹åŠ¿åˆå¹¶åˆ° news.json ä¸­
    
    è¶‹åŠ¿ä¼šä½œä¸ºç‰¹æ®Šçš„æ–°é—»é¡¹å‡ºç°åœ¨åˆ—è¡¨ä¸­ï¼Œ
    æ ‡è®° source='çƒ­ç‚¹å‘ç°' ä»¥åŒºåˆ†å¸¸è§„æ–°é—»
    """
    if not trends:
        return
    
    # åŠ è½½ç°æœ‰æ–°é—»
    news_items = []
    if NEWS_FILE.exists():
        try:
            with open(NEWS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                news_items = data.get('items', [])
        except (json.JSONDecodeError, IOError):
            pass
    
    # ç§»é™¤æ—§çš„å‘ç°è¶‹åŠ¿
    news_items = [n for n in news_items if n.get('source') != 'ğŸ”¬ çƒ­ç‚¹å‘ç°']
    
    # æ·»åŠ æ–°è¶‹åŠ¿
    for trend in trends[:30]:  # æœ€å¤š30ä¸ª
        if trend.heat_score < 10:
            continue  # è¿‡æ»¤ä½çƒ­åº¦
        
        # æ„å»ºæ‘˜è¦
        parts = []
        if trend.is_burst:
            parts.append('âš¡ çªå‘çƒ­ç‚¹')
        parts.append(f'ğŸ”¥ çƒ­åŠ›å€¼: {trend.heat_score:.0f}')
        parts.append(f'ğŸ“Š é¢‘ç‡: {trend.frequency}')
        if trend.platforms:
            parts.append(f'ğŸ“± {",".join(trend.platforms[:3])}')
        if trend.macd_signal == 'bullish':
            parts.append('ğŸ“ˆ è¶‹åŠ¿ä¸Šå‡')
        if trend.related_titles:
            parts.append(f'ç›¸å…³: {trend.related_titles[0][:50]}')
        
        importance = 3
        if trend.is_burst:
            importance = 5
        elif trend.heat_score >= 60:
            importance = 4
        elif trend.heat_score >= 30:
            importance = 3
        
        import hashlib
        news_item = {
            'id': f"trend_{hashlib.md5(trend.keyword.encode()).hexdigest()[:10]}",
            'title': f"{trend.trend_direction} {trend.keyword}",
            'summary': ' Â· '.join(parts),
            'link': '',
            'source': 'ğŸ”¬ çƒ­ç‚¹å‘ç°',
            'source_icon': 'ğŸ”¬',
            'category': trend.category or 'æ—¶äº‹',
            'lang': 'zh',
            'image': '',
            'pub_date': trend.peak_time or datetime.now(timezone.utc).isoformat(),
            'fetch_time': datetime.now(timezone.utc).isoformat(),
            'importance': importance,
            'regions': [],
            'priority': 0,  # é«˜ä¼˜å…ˆçº§
            'hot_value': int(trend.heat_score * 1000),
            'is_discovered_trend': True,
            'trend_data': {
                'heat_score': trend.heat_score,
                'frequency': trend.frequency,
                'acceleration': trend.acceleration,
                'is_burst': trend.is_burst,
                'z_score': trend.burst_z_score,
                'macd_signal': trend.macd_signal,
                'direction': trend.trend_direction,
                'platforms': trend.platforms,
                'sparkline': trend.sparkline[-20:],
            }
        }
        news_items.append(news_item)
    
    # ä¿å­˜
    output = {
        'last_update': datetime.now(timezone.utc).isoformat(),
        'total': len(news_items),
        'sources': len(set(n.get('source', '') for n in news_items)),
        'items': news_items
    }
    
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    with open(NEWS_FILE, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    trend_count = sum(1 for n in news_items if n.get('is_discovered_trend'))
    print(f"  ğŸ“° å·²åˆå¹¶ {trend_count} ä¸ªè¶‹åŠ¿åˆ°æ–°é—»æ•°æ®")


# ==================== CLI å…¥å£ ====================
if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(
        description='ğŸ”¬ æŠ–éŸ³/å°çº¢ä¹¦ å®æ—¶çƒ­ç‚¹å‘ç°ç³»ç»Ÿ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å•æ¬¡è¿è¡Œ (å…¨å¹³å°)
  python3 scripts/discover_trends.py
  
  # å¾ªç¯è¿è¡Œ (æ¯10åˆ†é’Ÿ)
  python3 scripts/discover_trends.py --loop 10
  
  # åªé‡‡é›†ç‰¹å®šå¹³å°
  python3 scripts/discover_trends.py --platforms weibo,bilibili,zhihu
  
  # å¢åŠ ç§å­å…³é”®è¯æ•°é‡
  python3 scripts/discover_trends.py --keywords 20
  
  # åŒæ—¶è¿è¡Œæ–°é—»æŠ“å–
  python3 scripts/discover_trends.py --with-news

ç®—æ³•è¯´æ˜:
  çƒ­åŠ›å€¼å…¬å¼: H(t) = Î±Â·F(t)Â·e^{-Î»Î”t} + Î²Â·A(t) + Î³Â·S(t) + Î´Â·E(t)
  çªå‘æ£€æµ‹: Z-Score > 2.5 + MACDé‡‘å‰
  è¡°å‡æ¨¡å‹: Newtonå†·å´å®šå¾‹, åŠè¡°æœŸ4å°æ—¶
        """
    )
    
    parser.add_argument('--loop', type=int, default=0,
                        help='å¾ªç¯è¿è¡Œé—´éš”(åˆ†é’Ÿ), 0=å•æ¬¡è¿è¡Œ')
    parser.add_argument('--platforms', type=str, default='',
                        help='é‡‡é›†å¹³å°(é€—å·åˆ†éš”): bilibili,baidu,xiaohongshu,weibo,zhihu,douyin (é»˜è®¤: bilibili,baidu,xiaohongshu)')
    parser.add_argument('--keywords', type=int, default=10,
                        help='ç§å­å…³é”®è¯æ•°é‡ (é»˜è®¤10)')
    parser.add_argument('--topk', type=int, default=50,
                        help='è¾“å‡º Top-K è¶‹åŠ¿ (é»˜è®¤50)')
    parser.add_argument('--proxy', type=str, default='',
                        help='HTTPä»£ç† (å¦‚ http://127.0.0.1:7890)')
    parser.add_argument('--with-news', action='store_true',
                        help='åŒæ—¶è¿è¡Œæ–°é—»æŠ“å– (fetch_news.py)')
    parser.add_argument('--verbose', action='store_true',
                        help='è¯¦ç»†æ—¥å¿—')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # å®‰è£…ä¾èµ–
    try:
        import jieba
    except ImportError:
        print("ğŸ“¦ å®‰è£… jieba ä¸­æ–‡åˆ†è¯åº“...")
        os.system(f"{sys.executable} -m pip install jieba -q")
    
    try:
        import requests
    except ImportError:
        print("ğŸ“¦ å®‰è£… requests...")
        os.system(f"{sys.executable} -m pip install requests -q")
    
    # è§£æå¹³å°å‚æ•°ï¼ˆé»˜è®¤åªç”¨å¯é å¹³å°ï¼‰
    platforms = None
    if args.platforms:
        platforms = [p.strip() for p in args.platforms.split(',') if p.strip()]
    else:
        # é»˜è®¤åªç”¨ä¸éœ€è¦ç™»å½•çš„å¯é å¹³å°
        platforms = ['bilibili', 'baidu', 'xiaohongshu']
    
    if args.loop > 0:
        print(f"ğŸ”„ å¾ªç¯æ¨¡å¼: æ¯ {args.loop} åˆ†é’Ÿè¿è¡Œä¸€æ¬¡ (Ctrl+C é€€å‡º)")
        while True:
            try:
                discover_trends(
                    platforms=platforms,
                    keyword_count=args.keywords,
                    topK=args.topk,
                    proxy=args.proxy or None
                )
                
                # åŒæ—¶è¿è¡Œæ–°é—»æŠ“å–
                if args.with_news:
                    print("\nğŸ“° è¿è¡Œæ–°é—»æŠ“å–...")
                    os.system(f"{sys.executable} {Path(__file__).parent / 'fetch_news.py'}")
                
                next_run = (datetime.now() + timedelta(minutes=args.loop)).strftime('%H:%M:%S')
                print(f"\nâ° ä¸‹æ¬¡è¿è¡Œ: {next_run}")
                time.sleep(args.loop * 60)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å·²åœæ­¢")
                break
            except Exception as e:
                print(f"\nâŒ å‡ºé”™: {e}")
                traceback.print_exc()
                time.sleep(60)
    else:
        trends = discover_trends(
            platforms=platforms,
            keyword_count=args.keywords,
            topK=args.topk,
            proxy=args.proxy or None
        )
        
        if args.with_news:
            print("\nğŸ“° è¿è¡Œæ–°é—»æŠ“å–...")
            os.system(f"{sys.executable} {Path(__file__).parent / 'fetch_news.py'}")
        
        print("\nâœ… å®Œæˆ!")
