#!/usr/bin/env python3
"""
æ¨¡å—ä¸€ï¼šæ•°æ®é‡‡é›†æ¶æ„ä¸åçˆ¬ç­–ç•¥
============================================================
æ ¸å¿ƒç†å¿µï¼šç»ä¸ä¾èµ–å®˜æ–¹"çƒ­æœæ¦œ/çƒ­ç‚¹æ¦œ"ï¼Œè€Œæ˜¯é€šè¿‡æŠ“å–ä¿¡æ¯æµã€æœç´¢ç»“æœã€
è¯é¢˜é¡µé¢ç­‰åº•å±‚æ•°æ®ï¼Œç”± NLP + Burst Detection è‡ªè¡Œè®¡ç®—çƒ­ç‚¹ã€‚

é‡‡é›†ç­–ç•¥ï¼š
1. ç§å­å…³é”®è¯è½®è¯¢æœç´¢ â†’ è·å–æœ€æ–°å†…å®¹æ ·æœ¬
2. å…¬å¼€è¯é¢˜/æ ‡ç­¾é¡µé¢ â†’ è·å–å‚ç±»å†…å®¹æµ
3. KOL ç§å­ç”¨æˆ·åŠ¨æ€ â†’ å‘ç°å¤´éƒ¨å†…å®¹
4. å¤šå¹³å°äº¤å‰éªŒè¯ â†’ é™ä½å•æºé£é™©

åçˆ¬ç­–ç•¥ï¼š
- User-Agent æ± è½®è½¬ (50+ çœŸå®æµè§ˆå™¨æŒ‡çº¹)
- è¯·æ±‚é¢‘ç‡æ§åˆ¶ (ä»¤ç‰Œæ¡¶é™æµ)
- æŒ‡æ•°é€€é¿é‡è¯• (429/403 è‡ªåŠ¨é™é€Ÿ)
- Session/Cookie å¤ç”¨ä¸åˆ·æ–°
- å¯é€‰ä»£ç†æ± æ”¯æŒ
- éšæœºå»¶è¿Ÿ jitter (æ¨¡æ‹Ÿäººç±»è¡Œä¸º)
"""

import json
import os
import re
import time
import random
import hashlib
import logging
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, field, asdict

# ==================== æ—¥å¿—é…ç½® ====================
logger = logging.getLogger('feed_crawler')

# ==================== æ•°æ®ç»“æ„ ====================
@dataclass
class RawContent:
    """é‡‡é›†åˆ°çš„åŸå§‹å†…å®¹"""
    platform: str           # æ¥æºå¹³å°: douyin / xiaohongshu / weibo / zhihu / bilibili
    content_id: str          # å”¯ä¸€æ ‡è¯†
    title: str               # æ ‡é¢˜/è¯é¢˜
    text: str                # æ­£æ–‡/æè¿°
    author: str = ''         # ä½œè€…
    likes: int = 0           # ç‚¹èµæ•°
    comments: int = 0        # è¯„è®ºæ•°
    shares: int = 0          # åˆ†äº«/è½¬å‘æ•°
    views: int = 0           # æ’­æ”¾/é˜…è¯»é‡
    tags: List[str] = field(default_factory=list)   # è¯é¢˜æ ‡ç­¾
    url: str = ''            # åŸå§‹é“¾æ¥
    pub_time: str = ''       # å‘å¸ƒæ—¶é—´ ISO
    crawl_time: str = ''     # é‡‡é›†æ—¶é—´ ISO
    content_type: str = ''   # video / note / article / answer
    extra: Dict = field(default_factory=dict)       # å¹³å°ç‰¹æœ‰å­—æ®µ

    def engagement_score(self) -> float:
        """äº’åŠ¨åŠ æƒå¾—åˆ†ï¼ˆæ ‡å‡†åŒ–äº’åŠ¨é‡ï¼‰"""
        return self.likes * 1.0 + self.comments * 3.0 + self.shares * 5.0 + self.views * 0.01

    def to_dict(self):
        return asdict(self)


# ==================== åçˆ¬åŸºç¡€è®¾æ–½ ====================
# 50+ çœŸå®æµè§ˆå™¨ User-Agent æ± 
UA_POOL = [
    # Chrome macOS
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    # Chrome Windows
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    # Firefox
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:122.0) Gecko/20100101 Firefox/122.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
    'Mozilla/5.0 (X11; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0',
    # Safari
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
    # Edge
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0',
    # Mobile
    'Mozilla/5.0 (iPhone; CPU iPhone OS 17_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1',
    'Mozilla/5.0 (Linux; Android 14; Pixel 8) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Mobile Safari/537.36',
    'Mozilla/5.0 (Linux; Android 14; SM-S918B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
    'Mozilla/5.0 (iPad; CPU OS 17_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1',
]

# ç§å­å…³é”®è¯åº“ï¼ˆæŒ‰é¢†åŸŸåˆ†ç»„ï¼Œæ¯æ¬¡éšæœºæŠ½æ ·ï¼‰
SEED_KEYWORDS = {
    'è´¢ç»': [
        'Aè‚¡', 'æ¸¯è‚¡', 'ç¾è‚¡', 'åŸºé‡‘', 'ç†è´¢', 'æŠ•èµ„', 'è‚¡ç¥¨', 'ä¸Šå¸‚', 'æ¶¨åœ', 'è·Œåœ',
        'å¤®è¡Œ', 'åˆ©ç‡', 'GDP', 'é€šèƒ€', 'é™æ¯', 'åŠ æ¯', 'æ¯”ç‰¹å¸', 'æ•°å­—è´§å¸', 'é»„é‡‘',
        'çŸ³æ²¹', 'æˆ¿ä»·', 'æ¥¼å¸‚', 'ç»æµ', 'é‡‘è', 'é“¶è¡Œ', 'ä¿é™©', 'æœŸè´§', 'å¤–æ±‡',
        'èèµ„', 'å¹¶è´­', 'åˆ›ä¸š', 'IPO', 'ç‹¬è§’å…½', 'æ–°èƒ½æº', 'å…‰ä¼', 'é”‚ç”µæ± ',
    ],
    'æ”¿æ²»': [
        'ä¸¤ä¼š', 'æ”¿ç­–', 'æ”¹é©', 'å¤–äº¤', 'åˆ¶è£', 'é€‰ä¸¾', 'ç«‹æ³•', 'å›½åŠ¡é™¢', 'äººå¤§',
        'å›½é˜²', 'å†›äº‹', 'å°æ¹¾', 'å—æµ·', 'ä¸€å¸¦ä¸€è·¯', 'ä¸­ç¾', 'ä¸­ä¿„', 'åŒ—çº¦',
        'è”åˆå›½', 'å³°ä¼š', 'æ€»ç»Ÿ', 'é¢†å¯¼äºº', 'åè®®', 'æ¡çº¦',
    ],
    'ç§‘æŠ€': [
        'AI', 'äººå·¥æ™ºèƒ½', 'å¤§æ¨¡å‹', 'ChatGPT', 'DeepSeek', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“',
        '5G', '6G', 'è‡ªåŠ¨é©¾é©¶', 'æœºå™¨äºº', 'é‡å­è®¡ç®—', 'èˆªå¤©', 'ç«ç®­', 'å«æ˜Ÿ',
        'åä¸º', 'è‹¹æœ', 'ç‰¹æ–¯æ‹‰', 'å°ç±³', 'æ‰‹æœº', 'æ–°å“', 'å‘å¸ƒä¼š',
        'åŒºå—é“¾', 'Web3', 'å…ƒå®‡å®™', 'AGI', 'Sora', 'è§†è§‰å¤§æ¨¡å‹',
    ],
    'ç¤¾ä¼š': [
        'æ•™è‚²', 'åŒ»ç–—', 'å°±ä¸š', 'æˆ¿ä»·', 'æˆ¿ç§Ÿ', 'ç”Ÿè‚²', 'å…»è€', 'é€€ä¼‘',
        'è€ƒå…¬', 'è€ƒç ”', 'é«˜è€ƒ', 'å†…å·', 'è£å‘˜', 'é™è–ª', 'è·³æ§½',
        'æ¶ˆè´¹', 'ç‰©ä»·', 'æ—…æ¸¸', 'æ˜¥è¿', 'ç”µå½±', 'ç»¼è‰º', 'çƒ­å‰§',
    ],
}


class RateLimiter:
    """
    ä»¤ç‰Œæ¡¶é™æµå™¨
    
    æ§åˆ¶æ¯ä¸ªåŸŸåçš„è¯·æ±‚é¢‘ç‡ï¼Œé˜²æ­¢è§¦å‘åçˆ¬ã€‚
    æ”¯æŒè‡ªé€‚åº”é™é€Ÿï¼šé‡åˆ°429/403æ—¶è‡ªåŠ¨æ‰©å¤§é—´éš”ã€‚
    æ”¯æŒåŸŸåå°ç¦ï¼šæ°¸ä¹…403/401è‡ªåŠ¨æ ‡è®°ï¼Œè·³è¿‡åç»­è¯·æ±‚ã€‚
    """
    def __init__(self, default_interval: float = 3.0, jitter: float = 2.0):
        self.default_interval = default_interval
        self.jitter = jitter
        self._last_request: Dict[str, float] = {}
        self._penalties: Dict[str, float] = {}  # åŸŸåæƒ©ç½šå€æ•°
        self._blocked: Dict[str, str] = {}  # åŸŸå â†’ å°ç¦åŸå› 
        self._fail_count: Dict[str, int] = {}  # åŸŸåè¿ç»­å¤±è´¥æ¬¡æ•°

    def is_blocked(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæ˜¯å¦å·²è¢«æ ‡è®°ä¸ºä¸å¯ç”¨"""
        return domain in self._blocked

    def block(self, domain: str, reason: str = 'unknown'):
        """æ ‡è®°åŸŸåä¸ºä¸å¯ç”¨ï¼ˆæœ¬æ¬¡è¿è¡ŒæœŸé—´è·³è¿‡æ‰€æœ‰è¯·æ±‚ï¼‰"""
        if domain not in self._blocked:
            self._blocked[domain] = reason
            logger.warning(f"  ğŸš« {domain} å·²æ ‡è®°ä¸ºä¸å¯ç”¨: {reason}")

    def record_fail(self, domain: str) -> int:
        """è®°å½•è¿ç»­å¤±è´¥æ¬¡æ•°ï¼Œè¿”å›å½“å‰æ¬¡æ•°"""
        self._fail_count[domain] = self._fail_count.get(domain, 0) + 1
        return self._fail_count[domain]

    def reset_fail(self, domain: str):
        """é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°"""
        self._fail_count[domain] = 0

    def wait(self, domain: str):
        """ç­‰å¾…ç›´åˆ°å¯ä»¥å‘é€ä¸‹ä¸€ä¸ªè¯·æ±‚"""
        if self.is_blocked(domain):
            return  # è¢«å°ç¦çš„åŸŸåä¸ç­‰å¾…ï¼Œç›´æ¥è·³è¿‡
        
        now = time.time()
        penalty = self._penalties.get(domain, 1.0)
        interval = self.default_interval * penalty + random.uniform(0, self.jitter)
        
        last = self._last_request.get(domain, 0)
        elapsed = now - last
        
        if elapsed < interval:
            sleep_time = interval - elapsed
            logger.debug(f"  â³ é™æµç­‰å¾… {sleep_time:.1f}s ({domain})")
            time.sleep(sleep_time)
        
        self._last_request[domain] = time.time()

    def penalize(self, domain: str, factor: float = 2.0):
        """å¯¹æŸåŸŸåæ–½åŠ æƒ©ç½šï¼ˆé™é€Ÿï¼‰"""
        current = self._penalties.get(domain, 1.0)
        self._penalties[domain] = min(current * factor, 5.0)  # æœ€å¤š5å€
        logger.warning(f"  âš ï¸ {domain} é™é€Ÿ â†’ é—´éš”Ã—{self._penalties[domain]:.1f}")

    def reset_penalty(self, domain: str):
        """é‡ç½®æƒ©ç½š"""
        if domain in self._penalties:
            self._penalties[domain] = max(1.0, self._penalties[domain] * 0.5)


# å…¨å±€é™æµå™¨
rate_limiter = RateLimiter(default_interval=2.5, jitter=2.0)


class BaseCrawler:
    """
    çˆ¬è™«åŸºç±» - æä¾›é€šç”¨åçˆ¬èƒ½åŠ›
    
    åŠŸèƒ½ï¼š
    1. Session ç®¡ç†ä¸ Cookie å¤ç”¨
    2. User-Agent æ™ºèƒ½è½®è½¬
    3. æŒ‡æ•°é€€é¿é‡è¯•
    4. å¯é€‰ä»£ç†æ”¯æŒ
    5. é”™è¯¯ç»Ÿè®¡ä¸è‡ªé€‚åº”é™é€Ÿ
    """
    
    def __init__(self, platform: str, proxy: str = None):
        import requests
        self.platform = platform
        self.proxy = proxy
        self.session = requests.Session()
        self._request_count = 0
        self._error_count = 0
        self._ua_index = random.randint(0, len(UA_POOL) - 1)
        
        # åŸºç¡€ headers
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
        })
        
        if proxy:
            self.session.proxies = {'http': proxy, 'https': proxy}

    def _rotate_ua(self) -> str:
        """è½®è½¬ User-Agent"""
        self._ua_index = (self._ua_index + random.randint(1, 5)) % len(UA_POOL)
        ua = UA_POOL[self._ua_index]
        self.session.headers['User-Agent'] = ua
        return ua

    def _get_domain(self, url: str) -> str:
        """æå–åŸŸåç”¨äºé™æµ"""
        from urllib.parse import urlparse
        return urlparse(url).netloc

    def safe_request(self, url: str, method: str = 'GET', max_retries: int = 3, 
                     timeout: int = 15, **kwargs) -> Optional[object]:
        """
        å®‰å…¨è¯·æ±‚ - é›†æˆé™æµã€é‡è¯•ã€åçˆ¬
        
        ç‰¹æ€§ï¼š
        - åŸŸåå°ç¦æ£€æµ‹ (403/401 æ°¸ä¹…è·³è¿‡)
        - è‡ªåŠ¨é™æµ (ä»¤ç‰Œæ¡¶)
        - è‡ªåŠ¨è½®è½¬ UA
        - æŒ‡æ•°é€€é¿é‡è¯• (1s â†’ 2s â†’ 4s â†’ ...)
        - 429 è‡ªåŠ¨é™é€Ÿ (ä»…å¯¹ä¸´æ—¶é™æµé‡è¯•)
        """
        domain = self._get_domain(url)
        
        # æ£€æŸ¥åŸŸåæ˜¯å¦å·²è¢«å°ç¦
        if rate_limiter.is_blocked(domain):
            logger.debug(f"  â­ï¸ è·³è¿‡å·²å°ç¦åŸŸå: {domain}")
            return None
        
        for attempt in range(max_retries):
            try:
                # é™æµç­‰å¾…
                rate_limiter.wait(domain)
                
                # è½®è½¬ UA
                self._rotate_ua()
                
                # å‘é€è¯·æ±‚
                self._request_count += 1
                resp = self.session.request(method, url, timeout=timeout, **kwargs)
                
                # === 401 Unauthorized: éœ€è¦ç™»å½•ï¼Œæ°¸ä¹…è·³è¿‡ ===
                if resp.status_code == 401:
                    fails = rate_limiter.record_fail(domain)
                    logger.warning(f"  ğŸ”’ 401 éœ€è¦ç™»å½• â†’ {domain} (ç¬¬{fails}æ¬¡)")
                    if fails >= 1:  # 401 ä¸€æ¬¡å°±å°ç¦
                        rate_limiter.block(domain, '401 éœ€è¦ç™»å½•è®¤è¯')
                    return None  # ä¸é‡è¯•
                
                # === 403 Forbidden: å¯èƒ½æ˜¯æ°¸ä¹…å°ç¦æˆ–ä¸´æ—¶é™æµ ===
                if resp.status_code == 403:
                    fails = rate_limiter.record_fail(domain)
                    # æ£€æŸ¥æ˜¯å¦æœ‰ Retry-After å¤´ï¼ˆè¯´æ˜æ˜¯ä¸´æ—¶é™æµï¼‰
                    retry_after = resp.headers.get('Retry-After')
                    if retry_after and attempt < max_retries - 1:
                        # æœ‰ Retry-After = ä¸´æ—¶é™æµï¼Œç­‰å¾…åé‡è¯•
                        wait = min(int(retry_after), 30)
                        logger.warning(f"  â³ 403 ä¸´æ—¶é™æµ â†’ ç­‰å¾… {wait}s")
                        time.sleep(wait)
                        continue
                    elif fails >= 2:
                        # è¿ç»­2æ¬¡403ä¸”æ— Retry-After â†’ æ°¸ä¹…å°ç¦
                        rate_limiter.block(domain, '403 æ‹’ç»è®¿é—®(éœ€è¦Cookie/ç­¾å)')
                        return None
                    else:
                        # ç¬¬ä¸€æ¬¡403ï¼ŒçŸ­æš‚ç­‰å¾…åé‡è¯•ä¸€æ¬¡
                        wait = 2 + random.uniform(0, 2)
                        logger.warning(f"  ğŸš« 403 Forbidden â†’ ç­‰å¾… {wait:.0f}s")
                        time.sleep(wait)
                        continue
                
                # === 429 Too Many Requests: ä¸´æ—¶é™æµï¼Œé™é€Ÿé‡è¯• ===
                if resp.status_code == 429:
                    rate_limiter.penalize(domain, 2.0)
                    wait = 2 ** (attempt + 1) + random.uniform(1, 3)
                    logger.warning(f"  ğŸš« 429 é™æµ â†’ ç­‰å¾… {wait:.0f}s")
                    time.sleep(wait)
                    continue
                
                # === 412 é£æ§è§¦å‘ ===
                if resp.status_code == 412:
                    fails = rate_limiter.record_fail(domain)
                    if fails >= 2:
                        rate_limiter.block(domain, '412 é£æ§è§¦å‘')
                        return None
                    rate_limiter.penalize(domain, 3.0)
                    logger.warning(f"  ğŸ›¡ï¸ 412 é£æ§è§¦å‘ â†’ é™é€Ÿ")
                    time.sleep(5 + random.uniform(0, 5))
                    continue
                
                resp.raise_for_status()
                rate_limiter.reset_penalty(domain)
                rate_limiter.reset_fail(domain)
                return resp
                
            except Exception as e:
                self._error_count += 1
                wait = 2 ** attempt + random.uniform(0, 2)
                if attempt < max_retries - 1:
                    logger.debug(f"  âš ï¸ è¯·æ±‚å¤±è´¥ [{attempt+1}/{max_retries}]: {str(e)[:60]} â†’ é‡è¯•")
                    time.sleep(wait)
                else:
                    logger.error(f"  âŒ è¯·æ±‚æœ€ç»ˆå¤±è´¥: {str(e)[:80]}")
        
        return None

    def is_domain_blocked(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæ˜¯å¦å·²è¢«å°ç¦"""
        return rate_limiter.is_blocked(domain)

    def stats(self) -> Dict:
        """è¿”å›é‡‡é›†ç»Ÿè®¡"""
        return {
            'platform': self.platform,
            'requests': self._request_count,
            'errors': self._error_count,
            'error_rate': f"{self._error_count/max(1,self._request_count)*100:.1f}%"
        }


# ==================== æŠ–éŸ³å†…å®¹é‡‡é›†å™¨ ====================
class DouyinCrawler(BaseCrawler):
    """
    æŠ–éŸ³ Feed æµé‡‡é›†å™¨
    
    é‡‡é›†ç­–ç•¥ï¼ˆä¸ä½¿ç”¨çƒ­æœæ¦œ APIï¼‰ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ç­–ç•¥1: æœç´¢å»ºè®®è¯ â†’ å‘ç°ç”¨æˆ·å®æ—¶æœç´¢è¶‹åŠ¿              â”‚
    â”‚ ç­–ç•¥2: å…³é”®è¯æœç´¢ â†’ è·å–æœ€æ–°è§†é¢‘æ ‡é¢˜+æè¿°+äº’åŠ¨æ•°æ®      â”‚
    â”‚ ç­–ç•¥3: è¯é¢˜/æŒ‘æˆ˜èµ›é¡µé¢ â†’ è·å–è¯é¢˜ä¸‹æœ€æ–°å†…å®¹             â”‚
    â”‚ ç­–ç•¥4: æ¨è Feed é‡‡æ · â†’ è·å–å¹³å°æ¨èå†…å®¹               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    åçˆ¬å¤‡æ³¨ï¼š
    - æŠ–éŸ³ Web ç«¯ä½¿ç”¨ a_bogus ç­¾åç®—æ³•ä¿æŠ¤ API
    - ç®€å• HTTP è¯·æ±‚å¯è·å–æœç´¢å»ºè®®è¯ã€éƒ¨åˆ†é¡µé¢ SSR æ•°æ®
    - å®Œæ•´ API è°ƒç”¨éœ€è¦é€†å‘ a_bogus (æœ¬æ¨¡å—æä¾›æ¡†æ¶ï¼Œéœ€é…åˆç­¾åæœåŠ¡)
    - é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨é¡µé¢ SSR æ•°æ® + æœç´¢å»ºè®®è¯
    """
    
    def __init__(self, proxy=None):
        super().__init__('douyin', proxy)
        self.session.headers.update({
            'Referer': 'https://www.douyin.com/',
        })

    def crawl_search_suggest(self, keyword: str) -> List[str]:
        """
        ç­–ç•¥1: æŠ–éŸ³æœç´¢å»ºè®®è¯ API
        
        åŸç†ï¼šè¾“å…¥éƒ¨åˆ†å…³é”®è¯ï¼Œè¿”å›ç”¨æˆ·å®æ—¶æœç´¢çƒ­è¯ã€‚
        è¿™äº›å»ºè®®è¯åæ˜ äº†å½“å‰ç”¨æˆ·çš„æœç´¢è¶‹åŠ¿ï¼Œæ— éœ€ç­¾åã€‚
        
        è¿”å›ï¼šå»ºè®®æœç´¢è¯åˆ—è¡¨
        """
        suggestions = []
        try:
            url = 'https://www.douyin.com/aweme/v1/web/search/sug/'
            params = {
                'keyword': keyword,
                'source': 'normal_search',
                'is_need_query': '1',
            }
            resp = self.safe_request(url, params=params)
            if resp and resp.status_code == 200:
                data = resp.json()
                for item in data.get('data', []):
                    word = item.get('content', '').strip()
                    if word and word != keyword:
                        suggestions.append(word)
                logger.info(f"  ğŸ” æŠ–éŸ³æœç´¢å»ºè®® [{keyword}]: {len(suggestions)} ä¸ªè¯")
        except Exception as e:
            logger.debug(f"  âš ï¸ æœç´¢å»ºè®®å¤±è´¥ [{keyword}]: {str(e)[:60]}")
        return suggestions

    def crawl_search_page(self, keyword: str) -> List[RawContent]:
        """
        ç­–ç•¥2: æŠ“å–æŠ–éŸ³æœç´¢ç»“æœé¡µé¢ (SSR)
        
        æŠ–éŸ³æœç´¢é¡µ URL: https://www.douyin.com/search/{keyword}
        é¡µé¢å¯èƒ½åŒ…å« SSR æ•°æ®ï¼ˆwindow.__RENDER_DATA__ï¼‰
        """
        items = []
        try:
            url = f'https://www.douyin.com/search/{keyword}'
            resp = self.safe_request(url, headers={'Accept': 'text/html'})
            if not resp:
                return items
            
            # å°è¯•æå– SSR æ•°æ®
            # æŠ–éŸ³ä½¿ç”¨ RENDER_DATA å­˜å‚¨ SSR æ•°æ® (URL encoded JSON)
            m = re.search(r'<script\s+id="RENDER_DATA"[^>]*>(.+?)</script>', resp.text, re.DOTALL)
            if m:
                import urllib.parse
                raw = urllib.parse.unquote(m.group(1))
                data = json.loads(raw)
                
                # é€’å½’æœç´¢ aweme æ•°æ®
                aweme_list = self._extract_aweme_list(data)
                for aweme in aweme_list[:20]:
                    content = self._parse_aweme(aweme, keyword)
                    if content:
                        items.append(content)
            
            # ä» HTML æå–è§†é¢‘ä¿¡æ¯ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
            if not items:
                items = self._parse_search_html(resp.text, keyword)
            
            logger.info(f"  ğŸµ æŠ–éŸ³æœç´¢ [{keyword}]: {len(items)} æ¡å†…å®¹")
        except Exception as e:
            logger.debug(f"  âš ï¸ æŠ–éŸ³æœç´¢é¡µå¤±è´¥ [{keyword}]: {str(e)[:60]}")
        return items

    def _extract_aweme_list(self, data: dict, depth: int = 0) -> list:
        """é€’å½’æå– aweme åˆ—è¡¨"""
        if depth > 10:
            return []
        results = []
        if isinstance(data, dict):
            if 'awemeList' in data:
                return data['awemeList'] if isinstance(data['awemeList'], list) else []
            if 'aweme_list' in data:
                return data['aweme_list'] if isinstance(data['aweme_list'], list) else []
            for v in data.values():
                results.extend(self._extract_aweme_list(v, depth + 1))
        elif isinstance(data, list):
            for item in data:
                if isinstance(item, dict):
                    results.extend(self._extract_aweme_list(item, depth + 1))
        return results

    def _parse_aweme(self, aweme: dict, keyword: str) -> Optional[RawContent]:
        """è§£æå•ä¸ª aweme å¯¹è±¡"""
        try:
            desc = aweme.get('desc', '').strip()
            if not desc:
                return None
            
            stats = aweme.get('statistics', aweme.get('stats', {}))
            author = aweme.get('author', {})
            
            # æå–è¯é¢˜æ ‡ç­¾
            tags = []
            text_extra = aweme.get('text_extra', [])
            for te in (text_extra if isinstance(text_extra, list) else []):
                ht = te.get('hashtag_name', '')
                if ht:
                    tags.append(ht)
            
            aweme_id = aweme.get('aweme_id', '') or aweme.get('id', '')
            
            return RawContent(
                platform='douyin',
                content_id=f"dy_{aweme_id}",
                title=desc[:100],
                text=desc,
                author=author.get('nickname', ''),
                likes=int(stats.get('digg_count', 0) or 0),
                comments=int(stats.get('comment_count', 0) or 0),
                shares=int(stats.get('share_count', 0) or 0),
                views=int(stats.get('play_count', 0) or 0),
                tags=tags,
                url=f'https://www.douyin.com/video/{aweme_id}' if aweme_id else '',
                pub_time=datetime.now(timezone.utc).isoformat(),
                crawl_time=datetime.now(timezone.utc).isoformat(),
                content_type='video',
                extra={'search_keyword': keyword}
            )
        except Exception:
            return None

    def _parse_search_html(self, html: str, keyword: str) -> List[RawContent]:
        """ä»æœç´¢é¡µ HTML æå–åŸºæœ¬ä¿¡æ¯ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        items = []
        # æå–é¡µé¢ä¸­çš„è§†é¢‘æ ‡é¢˜ï¼ˆmeta/og:title ç­‰ï¼‰
        titles = re.findall(r'<a[^>]*title="([^"]{5,})"[^>]*>', html)
        for i, title in enumerate(titles[:15]):
            title = title.strip()
            if len(title) < 5 or title in ('æœç´¢', 'é¦–é¡µ'):
                continue
            items.append(RawContent(
                platform='douyin',
                content_id=f"dy_html_{hashlib.md5(title.encode()).hexdigest()[:10]}",
                title=title,
                text=title,
                tags=[keyword],
                crawl_time=datetime.now(timezone.utc).isoformat(),
                content_type='video',
                extra={'search_keyword': keyword, 'parse_method': 'html'}
            ))
        return items

    def crawl_hashtag(self, hashtag: str) -> List[RawContent]:
        """
        ç­–ç•¥3: æŠ–éŸ³è¯é¢˜/æŒ‘æˆ˜èµ›é¡µé¢
        
        URL: https://www.douyin.com/hashtag/{hashtagId}
        é€šè¿‡è¯é¢˜é¡µé¢è·å–è¯¥è¯é¢˜ä¸‹çš„æœ€æ–°å†…å®¹
        """
        items = []
        try:
            url = f'https://www.douyin.com/search/{hashtag}?type=hashtag'
            resp = self.safe_request(url, headers={'Accept': 'text/html'})
            if resp:
                items = self._parse_search_html(resp.text, hashtag)
                logger.info(f"  #ï¸âƒ£ æŠ–éŸ³è¯é¢˜ [{hashtag}]: {len(items)} æ¡")
        except Exception as e:
            logger.debug(f"  âš ï¸ æŠ–éŸ³è¯é¢˜å¤±è´¥ [{hashtag}]: {str(e)[:60]}")
        return items

    def crawl_all(self, keywords: List[str] = None, max_keywords: int = 8) -> List[RawContent]:
        """
        ç»¼åˆé‡‡é›†ï¼šéšæœºé€‰å–ç§å­å…³é”®è¯ â†’ æœç´¢ + å»ºè®®è¯å‘ç°
        
        æµç¨‹ï¼š
        1. ä»ç§å­è¯åº“éšæœºæŠ½æ · max_keywords ä¸ªå…³é”®è¯
        2. å¯¹æ¯ä¸ªå…³é”®è¯ï¼šè·å–æœç´¢å»ºè®® + æœç´¢é¡µå†…å®¹
        3. ä»å»ºè®®è¯ä¸­å‘ç°æ–°çš„çƒ­é—¨è¯ï¼ˆäºŒçº§æ‰©å±•ï¼‰
        """
        all_items = []
        all_discovered_words = []
        
        if not keywords:
            # ä»æ‰€æœ‰é¢†åŸŸéšæœºæŠ½æ ·
            all_seeds = []
            for category_words in SEED_KEYWORDS.values():
                all_seeds.extend(category_words)
            keywords = random.sample(all_seeds, min(max_keywords, len(all_seeds)))
        
        logger.info(f"\nğŸµ æŠ–éŸ³é‡‡é›† [{len(keywords)} ä¸ªå…³é”®è¯]")
        
        for kw in keywords:
            if self.is_domain_blocked('www.douyin.com'):
                logger.info(f"  â­ï¸ æŠ–éŸ³åŸŸåå·²å°ç¦ï¼Œåœæ­¢é‡‡é›†")
                break
            # æœç´¢å»ºè®®è¯ï¼ˆå‘ç°ç”¨æˆ·å®æ—¶æœç´¢è¶‹åŠ¿ï¼‰
            suggestions = self.crawl_search_suggest(kw)
            all_discovered_words.extend(suggestions[:5])
            
            # æœç´¢é¡µå†…å®¹
            items = self.crawl_search_page(kw)
            all_items.extend(items)
        
        # äºŒçº§æ‰©å±•ï¼šå¯¹å‘ç°çš„çƒ­é—¨å»ºè®®è¯åšè¿›ä¸€æ­¥é‡‡é›†
        if all_discovered_words:
            expand_words = random.sample(all_discovered_words, 
                                         min(3, len(all_discovered_words)))
            for word in expand_words:
                items = self.crawl_search_page(word)
                all_items.extend(items)
        
        logger.info(f"  ğŸ“Š æŠ–éŸ³é‡‡é›†å®Œæˆ: {len(all_items)} æ¡å†…å®¹, "
                     f"å‘ç° {len(all_discovered_words)} ä¸ªè¶‹åŠ¿è¯")
        return all_items


# ==================== å°çº¢ä¹¦å†…å®¹é‡‡é›†å™¨ ====================
class XiaohongshuCrawler(BaseCrawler):
    """
    å°çº¢ä¹¦ Feed æµé‡‡é›†å™¨
    
    é‡‡é›†ç­–ç•¥ï¼ˆä¸ä½¿ç”¨çƒ­æœæ¦œï¼‰ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ç­–ç•¥1: Explore å‘ç°é¡µ â†’ è·å–æ¨èå†…å®¹æµ               â”‚
    â”‚ ç­–ç•¥2: å…³é”®è¯æœç´¢é¡µ â†’ è·å–ç‰¹å®šè¯é¢˜æœ€æ–°å†…å®¹            â”‚
    â”‚ ç­–ç•¥3: è¯é¢˜èšåˆé¡µ â†’ è·å–å‚ç±»å†…å®¹                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    åçˆ¬å¤‡æ³¨ï¼š
    - å°çº¢ä¹¦ API ä½¿ç”¨ X-Sign/shield ç­¾åä¿æŠ¤
    - Explore é¡µé¢åŒ…å« SSR æ•°æ® (window.__INITIAL_STATE__)
    - æœç´¢é¡µé¢ä¹Ÿå¯èƒ½åŒ…å« SSR æ•°æ®
    - SSR æ•°æ®ä¸­çš„ `undefined` éœ€æ›¿æ¢ä¸º `null`
    """
    
    def __init__(self, proxy=None):
        super().__init__('xiaohongshu', proxy)
        self.session.headers.update({
            'Referer': 'https://www.xiaohongshu.com/',
        })

    def crawl_explore(self) -> List[RawContent]:
        """
        ç­–ç•¥1: å°çº¢ä¹¦å‘ç°é¡µ (SSR)
        
        è§£æ window.__INITIAL_STATE__ ä¸­çš„ feed æ•°æ®
        åŒ…å«æ¨èå†…å®¹çš„æ ‡é¢˜ã€ä½œè€…ã€äº’åŠ¨æ•°æ®
        """
        items = []
        try:
            resp = self.safe_request('https://www.xiaohongshu.com/explore')
            if not resp:
                return items
            
            m = re.search(r'window\.__INITIAL_STATE__\s*=\s*(.+?)</script>', resp.text, re.DOTALL)
            if not m:
                logger.warning("  âŒ å°çº¢ä¹¦ Explore: æ— æ³•è§£æ SSR æ•°æ®")
                return items
            
            raw = m.group(1).strip().rstrip(';').replace('undefined', 'null')
            data = json.loads(raw)
            feeds = data.get('feed', {}).get('feeds', [])
            
            for entry in feeds[:30]:
                nc = entry.get('noteCard', entry)
                title = nc.get('displayTitle', '').strip()
                if not title:
                    continue
                
                user = nc.get('user', {})
                interact = nc.get('interactInfo', {})
                note_type = nc.get('type', 'normal')
                note_id = entry.get('id', '')
                
                likes_str = str(interact.get('likedCount', '0'))
                try:
                    likes = int(likes_str.replace('ä¸‡', '0000').replace('.', '').replace('+', ''))
                except ValueError:
                    likes = 0
                
                # æå–è¯é¢˜æ ‡ç­¾
                tags = []
                tag_list = nc.get('tagList', [])
                for t in (tag_list if isinstance(tag_list, list) else []):
                    tag_name = t.get('name', '') if isinstance(t, dict) else str(t)
                    if tag_name:
                        tags.append(tag_name)
                
                items.append(RawContent(
                    platform='xiaohongshu',
                    content_id=f"xhs_{note_id}",
                    title=title,
                    text=title,  # explore é¡µé¢é€šå¸¸åªæœ‰æ ‡é¢˜
                    author=user.get('nickname', ''),
                    likes=likes,
                    tags=tags,
                    url=f'https://www.xiaohongshu.com/explore/{note_id}' if note_id else '',
                    crawl_time=datetime.now(timezone.utc).isoformat(),
                    content_type='video' if note_type == 'video' else 'note',
                ))
            
            logger.info(f"  ğŸ“• å°çº¢ä¹¦ Explore: {len(items)} æ¡å†…å®¹")
        except Exception as e:
            logger.error(f"  âŒ å°çº¢ä¹¦ Explore å¤±è´¥: {str(e)[:80]}")
        return items

    def crawl_search(self, keyword: str) -> List[RawContent]:
        """
        ç­–ç•¥2: å°çº¢ä¹¦æœç´¢é¡µé¢ (SSR)
        
        URL: https://www.xiaohongshu.com/search_result?keyword={kw}
        """
        items = []
        try:
            import urllib.parse
            encoded_kw = urllib.parse.quote(keyword)
            url = f'https://www.xiaohongshu.com/search_result?keyword={encoded_kw}&source=web_search_result_notes'
            
            resp = self.safe_request(url, headers={'Accept': 'text/html'})
            if not resp:
                return items
            
            # å°è¯•è§£æ SSR æ•°æ®
            m = re.search(r'window\.__INITIAL_STATE__\s*=\s*(.+?)</script>', resp.text, re.DOTALL)
            if m:
                raw = m.group(1).strip().rstrip(';').replace('undefined', 'null')
                data = json.loads(raw)
                
                # æœç´¢ç»“æœåœ¨ search.notes æˆ– search.feeds ä¸­
                notes = (data.get('search', {}).get('notes', {}).get('items', []) or
                         data.get('search', {}).get('feeds', []))
                
                for entry in (notes[:20] if isinstance(notes, list) else []):
                    note = entry.get('noteCard', entry) if isinstance(entry, dict) else {}
                    title = note.get('displayTitle', '').strip()
                    if not title:
                        continue
                    
                    user = note.get('user', {}) if isinstance(note.get('user'), dict) else {}
                    interact = note.get('interactInfo', {}) if isinstance(note.get('interactInfo'), dict) else {}
                    note_id = entry.get('id', '') if isinstance(entry, dict) else ''
                    
                    likes_str = str(interact.get('likedCount', '0'))
                    try:
                        likes = int(likes_str.replace('ä¸‡', '0000').replace('.', '').replace('+', ''))
                    except ValueError:
                        likes = 0
                    
                    items.append(RawContent(
                        platform='xiaohongshu',
                        content_id=f"xhs_s_{hashlib.md5(title.encode()).hexdigest()[:10]}",
                        title=title,
                        text=title,
                        author=user.get('nickname', ''),
                        likes=likes,
                        tags=[keyword],
                        url=f'https://www.xiaohongshu.com/explore/{note_id}' if note_id else '',
                        crawl_time=datetime.now(timezone.utc).isoformat(),
                        content_type='note',
                        extra={'search_keyword': keyword}
                    ))
            
            # é™çº§ï¼šä» HTML æå–
            if not items:
                titles = re.findall(r'<a[^>]*class="[^"]*title[^"]*"[^>]*>([^<]{5,})</a>', resp.text)
                for title in titles[:15]:
                    title = title.strip()
                    items.append(RawContent(
                        platform='xiaohongshu',
                        content_id=f"xhs_h_{hashlib.md5(title.encode()).hexdigest()[:10]}",
                        title=title,
                        text=title,
                        tags=[keyword],
                        crawl_time=datetime.now(timezone.utc).isoformat(),
                        content_type='note',
                        extra={'search_keyword': keyword, 'parse_method': 'html'}
                    ))
            
            logger.info(f"  ğŸ” å°çº¢ä¹¦æœç´¢ [{keyword}]: {len(items)} æ¡å†…å®¹")
        except Exception as e:
            logger.debug(f"  âš ï¸ å°çº¢ä¹¦æœç´¢å¤±è´¥ [{keyword}]: {str(e)[:60]}")
        return items

    def crawl_all(self, keywords: List[str] = None, max_keywords: int = 8) -> List[RawContent]:
        """ç»¼åˆé‡‡é›†"""
        all_items = []
        
        if not keywords:
            all_seeds = []
            for category_words in SEED_KEYWORDS.values():
                all_seeds.extend(category_words)
            keywords = random.sample(all_seeds, min(max_keywords, len(all_seeds)))
        
        logger.info(f"\nğŸ“• å°çº¢ä¹¦é‡‡é›† [{len(keywords)} ä¸ªå…³é”®è¯]")
        
        # Explore é¡µé¢
        explore_items = self.crawl_explore()
        all_items.extend(explore_items)
        
        # å…³é”®è¯æœç´¢ï¼ˆè¿ç»­3æ¬¡ç©ºç»“æœåˆ™åœæ­¢ï¼‰
        empty_count = 0
        for kw in keywords:
            if self.is_domain_blocked('www.xiaohongshu.com'):
                logger.info(f"  â­ï¸ å°çº¢ä¹¦åŸŸåå·²å°ç¦ï¼Œåœæ­¢æœç´¢")
                break
            if empty_count >= 3:
                logger.info(f"  â­ï¸ å°çº¢ä¹¦æœç´¢è¿ç»­ç©ºç»“æœï¼Œè·³è¿‡å‰©ä½™å…³é”®è¯")
                break
            items = self.crawl_search(kw)
            if not items:
                empty_count += 1
            else:
                empty_count = 0
            all_items.extend(items)
        
        logger.info(f"  ğŸ“Š å°çº¢ä¹¦é‡‡é›†å®Œæˆ: {len(all_items)} æ¡å†…å®¹")
        return all_items


# ==================== è¡¥å……æ•°æ®æºï¼ˆäº¤å‰éªŒè¯ï¼‰ ====================
class WeiboCrawler(BaseCrawler):
    """
    å¾®åšå†…å®¹é‡‡é›†å™¨ (è¡¥å……æ•°æ®æº)
    
    å¾®åš AJAX æ¥å£ç›¸å¯¹å¼€æ”¾ï¼Œå¯è·å–å®æ—¶çƒ­ç‚¹å†…å®¹ã€‚
    ç”¨äºä¸æŠ–éŸ³/å°çº¢ä¹¦æ•°æ®äº¤å‰éªŒè¯ã€‚
    """
    
    def __init__(self, proxy=None):
        super().__init__('weibo', proxy)

    def crawl_realtime(self) -> List[RawContent]:
        """å¾®åšå®æ—¶çƒ­ç‚¹å†…å®¹"""
        items = []
        try:
            # å¾®åšçƒ­æœ AJAX (ä¸æ˜¯æŠ–éŸ³/å°çº¢ä¹¦çš„çƒ­æœï¼Œå…è®¸ä½¿ç”¨)
            resp = self.safe_request('https://weibo.com/ajax/side/hotSearch')
            if not resp:
                return items
            
            data = resp.json()
            realtime = data.get('data', {}).get('realtime', [])
            
            for entry in realtime[:30]:
                word = entry.get('word', '').strip()
                if not word:
                    continue
                
                items.append(RawContent(
                    platform='weibo',
                    content_id=f"wb_{hashlib.md5(word.encode()).hexdigest()[:10]}",
                    title=word,
                    text=entry.get('label_name', '') + ' ' + word,
                    views=int(entry.get('raw_hot', 0) or 0),
                    tags=[entry.get('category', '')],
                    url=f'https://s.weibo.com/weibo?q={word}',
                    crawl_time=datetime.now(timezone.utc).isoformat(),
                    content_type='topic',
                    extra={
                        'is_hot': entry.get('is_hot', 0),
                        'is_new': entry.get('is_new', 0),
                        'is_fei': entry.get('is_fei', 0),
                        'category': entry.get('category', ''),
                        'raw_hot': entry.get('raw_hot', 0),
                    }
                ))
            logger.info(f"  ğŸ¦ å¾®åšå®æ—¶: {len(items)} æ¡")
        except Exception as e:
            logger.error(f"  âŒ å¾®åšé‡‡é›†å¤±è´¥: {str(e)[:80]}")
        return items

    def crawl_topic_feed(self, topic: str) -> List[RawContent]:
        """å¾®åšè¯é¢˜ Feed"""
        items = []
        try:
            import urllib.parse
            url = f'https://weibo.com/ajax/statuses/topic?q={urllib.parse.quote(topic)}&count=20'
            resp = self.safe_request(url)
            if resp:
                data = resp.json()
                for status in data.get('data', {}).get('statuses', [])[:15]:
                    text = status.get('text_raw', status.get('text', '')).strip()
                    if not text:
                        continue
                    user = status.get('user', {})
                    items.append(RawContent(
                        platform='weibo',
                        content_id=f"wb_t_{status.get('id', '')}",
                        title=text[:100],
                        text=text,
                        author=user.get('screen_name', ''),
                        likes=int(status.get('attitudes_count', 0) or 0),
                        comments=int(status.get('comments_count', 0) or 0),
                        shares=int(status.get('reposts_count', 0) or 0),
                        tags=[topic],
                        crawl_time=datetime.now(timezone.utc).isoformat(),
                        content_type='status',
                    ))
            logger.info(f"  ğŸ¦ å¾®åšè¯é¢˜ [{topic}]: {len(items)} æ¡")
        except Exception as e:
            logger.debug(f"  âš ï¸ å¾®åšè¯é¢˜å¤±è´¥ [{topic}]: {str(e)[:60]}")
        return items

    def crawl_all(self, keywords=None, max_keywords=5) -> List[RawContent]:
        all_items = self.crawl_realtime()
        
        # å¦‚æœweibo.comå·²è¢«å°ç¦ï¼Œè·³è¿‡è¯é¢˜é‡‡é›†
        if self.is_domain_blocked('weibo.com'):
            logger.info(f"  â­ï¸ å¾®åšåŸŸåå·²å°ç¦ï¼Œè·³è¿‡è¯é¢˜é‡‡é›†")
            return all_items
        
        if keywords:
            for kw in keywords[:max_keywords]:
                if self.is_domain_blocked('weibo.com'):
                    break
                items = self.crawl_topic_feed(kw)
                all_items.extend(items)
        
        return all_items


class BilibiliCrawler(BaseCrawler):
    """Bç«™çƒ­é—¨è§†é¢‘é‡‡é›†"""
    
    def __init__(self, proxy=None):
        super().__init__('bilibili', proxy)

    def crawl_popular(self) -> List[RawContent]:
        """Bç«™çƒ­é—¨è§†é¢‘"""
        items = []
        try:
            resp = self.safe_request('https://api.bilibili.com/x/web-interface/popular?ps=50&pn=1')
            if not resp:
                return items
            
            data = resp.json()
            for entry in data.get('data', {}).get('list', [])[:30]:
                title = entry.get('title', '').strip()
                if not title:
                    continue
                
                stat = entry.get('stat', {})
                owner = entry.get('owner', {})
                
                items.append(RawContent(
                    platform='bilibili',
                    content_id=f"bl_{entry.get('bvid', '')}",
                    title=title,
                    text=entry.get('desc', title),
                    author=owner.get('name', ''),
                    likes=int(stat.get('like', 0)),
                    comments=int(stat.get('reply', 0)),
                    shares=int(stat.get('share', 0)),
                    views=int(stat.get('view', 0)),
                    url=f"https://www.bilibili.com/video/{entry.get('bvid', '')}",
                    crawl_time=datetime.now(timezone.utc).isoformat(),
                    content_type='video',
                ))
            logger.info(f"  ğŸ“º Bç«™çƒ­é—¨: {len(items)} æ¡")
        except Exception as e:
            logger.error(f"  âŒ Bç«™é‡‡é›†å¤±è´¥: {str(e)[:80]}")
        return items

    def crawl_all(self, keywords=None, max_keywords=5) -> List[RawContent]:
        return self.crawl_popular()


class ZhihuCrawler(BaseCrawler):
    """çŸ¥ä¹çƒ­é—¨é‡‡é›†"""
    
    def __init__(self, proxy=None):
        super().__init__('zhihu', proxy)
        self.session.headers.update({
            'Referer': 'https://www.zhihu.com/',
        })

    def crawl_hot(self) -> List[RawContent]:
        """çŸ¥ä¹çƒ­æ¦œ"""
        items = []
        try:
            resp = self.safe_request('https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total?limit=50')
            if not resp:
                return items
            
            data = resp.json()
            for entry in data.get('data', [])[:30]:
                target = entry.get('target', {})
                title = target.get('title', '').strip()
                if not title:
                    continue
                
                items.append(RawContent(
                    platform='zhihu',
                    content_id=f"zh_{target.get('id', '')}",
                    title=title,
                    text=target.get('excerpt', title),
                    views=int(entry.get('detail_text', '0').replace('ä¸‡çƒ­åº¦', '0000').replace('çƒ­åº¦', '').strip() or 0),
                    url=f"https://www.zhihu.com/question/{target.get('id', '')}",
                    crawl_time=datetime.now(timezone.utc).isoformat(),
                    content_type='question',
                    extra={'hot_text': entry.get('detail_text', '')}
                ))
            logger.info(f"  ğŸ’¬ çŸ¥ä¹çƒ­æ¦œ: {len(items)} æ¡")
        except Exception as e:
            logger.error(f"  âŒ çŸ¥ä¹é‡‡é›†å¤±è´¥: {str(e)[:80]}")
        return items

    def crawl_all(self, keywords=None, max_keywords=5) -> List[RawContent]:
        return self.crawl_hot()


class BaiduCrawler(BaseCrawler):
    """ç™¾åº¦çƒ­æœé‡‡é›†"""
    
    def __init__(self, proxy=None):
        super().__init__('baidu', proxy)

    def crawl_realtime(self) -> List[RawContent]:
        """ç™¾åº¦å®æ—¶çƒ­ç‚¹"""
        items = []
        try:
            # ç™¾åº¦éœ€è¦æ¡Œé¢UA + å®Œæ•´Acceptå¤´æ‰è¿”å›SSRæ•°æ®
            resp = self.safe_request('https://top.baidu.com/board?tab=realtime',
                                     headers={
                                         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                                         'Accept-Encoding': 'gzip, deflate',  # ä¸è¦brï¼Œé¿å…brotliè§£ç é—®é¢˜
                                         'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                                     })
            if not resp:
                return items
            
            # è§£æ SSR æ•°æ®
            m = re.search(r'<!--s-data:(.*?)-->', resp.text, re.DOTALL)
            if m:
                data = json.loads(m.group(1))
                cards = data.get('data', {}).get('cards', [])
                for card in cards:
                    for item in card.get('content', [])[:30]:
                        title = item.get('word', '').strip()
                        if not title:
                            continue
                        items.append(RawContent(
                            platform='baidu',
                            content_id=f"bd_{hashlib.md5(title.encode()).hexdigest()[:10]}",
                            title=title,
                            text=item.get('desc', title),
                            views=int(item.get('hotScore', 0) or 0),
                            url=item.get('url', ''),
                            crawl_time=datetime.now(timezone.utc).isoformat(),
                            content_type='search',
                        ))
            logger.info(f"  ğŸ” ç™¾åº¦çƒ­æœ: {len(items)} æ¡")
        except Exception as e:
            logger.error(f"  âŒ ç™¾åº¦é‡‡é›†å¤±è´¥: {str(e)[:80]}")
        return items

    def crawl_all(self, keywords=None, max_keywords=5) -> List[RawContent]:
        return self.crawl_realtime()


# ==================== é‡‡é›†ç¼–æ’å™¨ ====================
class CrawlOrchestrator:
    """
    é‡‡é›†ç¼–æ’å™¨ - è°ƒåº¦å¤šå¹³å°é‡‡é›†ä»»åŠ¡
    
    åŠŸèƒ½ï¼š
    1. åŠ¨æ€é€‰æ‹©ç§å­å…³é”®è¯ï¼ˆæ¯æ¬¡ä¸åŒï¼‰
    2. å¹¶å‘ç¼–æ’å¤šå¹³å°é‡‡é›†
    3. è‡ªé€‚åº”é‡‡é›†å¼ºåº¦ï¼ˆæ ¹æ®é”™è¯¯ç‡è°ƒæ•´ï¼‰
    4. åŸå§‹æ•°æ®è½ç›˜
    """
    
    def __init__(self, proxy: str = None, save_raw: bool = True):
        self.proxy = proxy
        self.save_raw = save_raw
        self.data_dir = Path(__file__).parent.parent / "data"
        self.raw_dir = self.data_dir / "raw_feeds"
        
        # åˆå§‹åŒ–å„å¹³å°çˆ¬è™«ï¼ˆå¯é å¹³å°ä¼˜å…ˆï¼‰
        self.crawlers = {
            'bilibili': BilibiliCrawler(proxy),
            'baidu': BaiduCrawler(proxy),
            'xiaohongshu': XiaohongshuCrawler(proxy),
            'weibo': WeiboCrawler(proxy),
            'zhihu': ZhihuCrawler(proxy),
            'douyin': DouyinCrawler(proxy),
        }

    def select_keywords(self, count: int = 10) -> List[str]:
        """
        åŠ¨æ€é€‰æ‹©ç§å­å…³é”®è¯
        
        ç­–ç•¥ï¼š
        - æ¯ä¸ªé¢†åŸŸè‡³å°‘é€‰ 2 ä¸ªï¼ˆä¿è¯è¦†ç›–é¢ï¼‰
        - æ€»æ•°æ§åˆ¶åœ¨ count ä»¥å†…
        - éšæœºåŒ–é˜²æ­¢è¢«è¯†åˆ«ä¸ºå›ºå®šé‡‡é›†æ¨¡å¼
        """
        selected = []
        categories = list(SEED_KEYWORDS.keys())
        per_cat = max(2, count // len(categories))
        
        for cat in categories:
            words = SEED_KEYWORDS[cat]
            chosen = random.sample(words, min(per_cat, len(words)))
            selected.extend(chosen)
        
        random.shuffle(selected)
        return selected[:count]

    def crawl_all(self, platforms: List[str] = None, 
                  keyword_count: int = 10) -> List[RawContent]:
        """
        æ‰§è¡Œå…¨å¹³å°é‡‡é›†
        
        Args:
            platforms: è¦é‡‡é›†çš„å¹³å°åˆ—è¡¨ï¼Œé»˜è®¤å…¨éƒ¨
            keyword_count: ç§å­å…³é”®è¯æ•°é‡
            
        Returns:
            æ‰€æœ‰å¹³å°çš„åŸå§‹å†…å®¹åˆ—è¡¨
        """
        if platforms is None:
            platforms = list(self.crawlers.keys())
        
        keywords = self.select_keywords(keyword_count)
        all_items: List[RawContent] = []
        stats = {}
        
        start_time = time.time()
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸŒ å¤šå¹³å°é‡‡é›†å¼€å§‹ [{datetime.now().strftime('%H:%M:%S')}]")
        logger.info(f"   å¹³å°: {', '.join(platforms)}")
        logger.info(f"   å…³é”®è¯: {', '.join(keywords[:5])}...")
        logger.info(f"{'='*60}")
        
        # å¹³å°åŸŸåæ˜ å°„ï¼ˆç”¨äºå°ç¦æ£€æŸ¥ï¼‰
        platform_domains = {
            'douyin': 'www.douyin.com',
            'xiaohongshu': 'www.xiaohongshu.com',
            'weibo': 'weibo.com',
            'bilibili': 'api.bilibili.com',
            'zhihu': 'www.zhihu.com',
            'baidu': 'top.baidu.com',
        }
        
        for platform in platforms:
            crawler = self.crawlers.get(platform)
            if not crawler:
                continue
            
            # æ£€æŸ¥åŸŸåæ˜¯å¦å·²è¢«å°ç¦ï¼Œè·³è¿‡æ•´ä¸ªå¹³å°
            domain = platform_domains.get(platform, '')
            if domain and rate_limiter.is_blocked(domain):
                logger.info(f"  â­ï¸ è·³è¿‡ {platform} (åŸŸåå·²å°ç¦)")
                stats[platform] = {'status': 'blocked', 'items': 0}
                continue
            
            try:
                platform_start = time.time()
                items = crawler.crawl_all(keywords=keywords)
                platform_elapsed = time.time() - platform_start
                all_items.extend(items)
                stats[platform] = crawler.stats()
                stats[platform]['items'] = len(items)
                stats[platform]['time'] = f"{platform_elapsed:.1f}s"
            except Exception as e:
                logger.error(f"  âŒ {platform} é‡‡é›†å¼‚å¸¸: {str(e)[:80]}")
                stats[platform] = {'error': str(e)[:80]}
        
        elapsed = time.time() - start_time
        
        # å»é‡
        seen = set()
        unique_items = []
        for item in all_items:
            key = item.title[:30].lower().replace(' ', '')
            if key not in seen:
                seen.add(key)
                unique_items.append(item)
        
        logger.info(f"\nğŸ“Š é‡‡é›†æ±‡æ€» ({elapsed:.1f}s):")
        logger.info(f"   æ€»è®¡: {len(all_items)} â†’ å»é‡å: {len(unique_items)}")
        for p, s in stats.items():
            logger.info(f"   {p}: {s}")
        
        # ä¿å­˜åŸå§‹æ•°æ®
        if self.save_raw:
            self._save_raw(unique_items)
        
        return unique_items

    def _save_raw(self, items: List[RawContent]):
        """ä¿å­˜åŸå§‹é‡‡é›†æ•°æ®ï¼ˆç”¨äºè¶‹åŠ¿åˆ†ææ—¶é—´åºåˆ—ï¼‰"""
        self.raw_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filepath = self.raw_dir / f"raw_{timestamp}.json"
        
        data = {
            'crawl_time': datetime.now(timezone.utc).isoformat(),
            'total': len(items),
            'items': [item.to_dict() for item in items]
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # æ¸…ç†7å¤©å‰çš„åŸå§‹æ•°æ®
        cutoff = datetime.now() - timedelta(days=7)
        for old_file in self.raw_dir.glob('raw_*.json'):
            try:
                fdate_str = old_file.stem.replace('raw_', '')
                fdate = datetime.strptime(fdate_str, '%Y%m%d_%H%M%S')
                if fdate < cutoff:
                    old_file.unlink()
            except (ValueError, OSError):
                pass
        
        logger.info(f"  ğŸ’¾ åŸå§‹æ•°æ®ä¿å­˜: {filepath.name}")


# ================================
# æ¨¡å—å¯¼å‡º
# ================================
__all__ = [
    'RawContent', 'CrawlOrchestrator',
    'DouyinCrawler', 'XiaohongshuCrawler',
    'WeiboCrawler', 'BilibiliCrawler', 'ZhihuCrawler', 'BaiduCrawler',
    'SEED_KEYWORDS', 'UA_POOL', 'RateLimiter',
]

if __name__ == '__main__':
    # ç‹¬ç«‹æµ‹è¯•
    logging.basicConfig(level=logging.INFO, format='%(message)s')
    
    orchestrator = CrawlOrchestrator()
    # ä¼˜å…ˆé‡‡é›†å¯é å¹³å°ï¼Œä¸å¯é çš„æ”¾æœ€å
    items = orchestrator.crawl_all(
        platforms=['bilibili', 'baidu', 'xiaohongshu', 'weibo', 'zhihu', 'douyin'],
        keyword_count=5
    )
    
    print(f"\nâœ… é‡‡é›†å®Œæˆ: {len(items)} æ¡å†…å®¹")
    for item in items[:10]:
        print(f"  [{item.platform}] {item.title[:50]}")
