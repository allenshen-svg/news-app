#!/usr/bin/env python3
"""
å…¨çƒæ—¶äº‹æ”¿ç»æ–°é—»èšåˆå™¨
ä»å¤šä¸ªRSSæºå’ŒAPIæŠ“å–æ–°é—»ï¼Œåˆ†ç±»æ•´ç†åè¾“å‡ºJSON
"""

import json
import os
import re
import hashlib
import time
import traceback
from datetime import datetime, timezone, timedelta
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==================== é…ç½® ====================
DATA_DIR = Path(__file__).parent.parent / "data"
OUTPUT_FILE = DATA_DIR / "news.json"
MAX_NEWS = 300  # æœ€å¤šä¿ç•™æ¡æ•°

# RSS æºé…ç½®
RSS_SOURCES = [
    # ---- å›½é™…ç»¼åˆ ----
    {
        "name": "Reuters World",
        "url": "https://feeds.reuters.com/Reuters/worldNews",
        "category": "å›½é™…",
        "lang": "en",
        "icon": "ğŸŒ",
        "priority": 1
    },
    {
        "name": "BBC World",
        "url": "http://feeds.bbci.co.uk/news/world/rss.xml",
        "category": "å›½é™…",
        "lang": "en",
        "icon": "ğŸ‡¬ğŸ‡§",
        "priority": 1
    },
    {
        "name": "Al Jazeera",
        "url": "https://www.aljazeera.com/xml/rss/all.xml",
        "category": "å›½é™…",
        "lang": "en",
        "icon": "ğŸŒ",
        "priority": 2
    },
    {
        "name": "NPR World",
        "url": "https://feeds.npr.org/1004/rss.xml",
        "category": "å›½é™…",
        "lang": "en",
        "icon": "ğŸ“»",
        "priority": 2
    },
    # ---- è´¢ç» ----
    {
        "name": "CNBC Top",
        "url": "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=100003114",
        "category": "è´¢ç»",
        "lang": "en",
        "icon": "ğŸ’¹",
        "priority": 1
    },
    {
        "name": "Bloomberg",
        "url": "https://feeds.bloomberg.com/markets/news.rss",
        "category": "è´¢ç»",
        "lang": "en",
        "icon": "ğŸ“Š",
        "priority": 1
    },
    {
        "name": "FT World",
        "url": "https://www.ft.com/rss/home",
        "category": "è´¢ç»",
        "lang": "en",
        "icon": "ğŸ‡¬ğŸ‡§",
        "priority": 2
    },
    # ---- æ”¿æ²» ----
    {
        "name": "BBC Politics",
        "url": "http://feeds.bbci.co.uk/news/politics/rss.xml",
        "category": "æ”¿æ²»",
        "lang": "en",
        "icon": "ğŸ›ï¸",
        "priority": 2
    },
    {
        "name": "Reuters Politics",
        "url": "https://feeds.reuters.com/Reuters/PoliticsNews",
        "category": "æ”¿æ²»",
        "lang": "en",
        "icon": "ğŸ—³ï¸",
        "priority": 1
    },
    # ---- ç§‘æŠ€ ----
    {
        "name": "Reuters Tech",
        "url": "https://feeds.reuters.com/reuters/technologyNews",
        "category": "ç§‘æŠ€",
        "lang": "en",
        "icon": "ğŸ’»",
        "priority": 2
    },
    {
        "name": "TechCrunch",
        "url": "https://techcrunch.com/feed/",
        "category": "ç§‘æŠ€",
        "lang": "en",
        "icon": "ğŸš€",
        "priority": 2
    },
    # ---- ä¸­æ–‡æº ----
    {
        "name": "æ–°æµªè´¢ç»",
        "url": "https://feed.mix.sina.com.cn/api/roll/get?pageid=153&lid=2516&k=&num=20&page=1&r=0.1&callback=",
        "category": "è´¢ç»",
        "lang": "zh",
        "icon": "ğŸ“ˆ",
        "priority": 1,
        "type": "sina_api"
    },
    {
        "name": "BBCä¸­æ–‡",
        "url": "https://feeds.bbci.co.uk/zhongwen/simp/rss.xml",
        "category": "å›½é™…",
        "lang": "zh",
        "icon": "ğŸ‡¬ğŸ‡§",
        "priority": 1
    },
    {
        "name": "çº½çº¦æ—¶æŠ¥ä¸­æ–‡",
        "url": "https://cn.nytimes.com/rss/",
        "category": "å›½é™…",
        "lang": "zh",
        "icon": "ğŸ‡ºğŸ‡¸",
        "priority": 1
    },
    {
        "name": "éŸ©è”ç¤¾ä¸­æ–‡",
        "url": "https://cn.yna.co.kr/RSS/news.xml",
        "category": "å›½é™…",
        "lang": "zh",
        "icon": "ğŸ‡°ğŸ‡·",
        "priority": 2
    },
    {
        "name": "DWå¾·å›½ä¹‹å£°",
        "url": "https://rss.dw.com/xml/rss-chi-all",
        "category": "å›½é™…",
        "lang": "zh",
        "icon": "ğŸ‡©ğŸ‡ª",
        "priority": 2
    },
    {
        "name": "Nikkei Asia",
        "url": "https://asia.nikkei.com/rss",
        "category": "è´¢ç»",
        "lang": "en",
        "icon": "ğŸ‡¯ğŸ‡µ",
        "priority": 2
    },
    {
        "name": "The Guardian World",
        "url": "https://www.theguardian.com/world/rss",
        "category": "å›½é™…",
        "lang": "en",
        "icon": "ğŸ‡¬ğŸ‡§",
        "priority": 2
    },
    {
        "name": "WSJ Markets",
        "url": "https://feeds.content.dowjones.io/public/rss/mw_topstories",
        "category": "è´¢ç»",
        "lang": "en",
        "icon": "ğŸ“Š",
        "priority": 1
    },
]

# ==================== å·¥å…·å‡½æ•° ====================

def clean_html(text):
    """å»é™¤HTMLæ ‡ç­¾"""
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'&[a-zA-Z]+;', ' ', text)
    text = re.sub(r'&#\d+;', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text[:500]  # é™åˆ¶é•¿åº¦

def make_id(title, source):
    """ç”Ÿæˆå”¯ä¸€ID"""
    raw = f"{title}_{source}"
    return hashlib.md5(raw.encode()).hexdigest()[:12]

def parse_date(date_str):
    """è§£æå„ç§æ—¥æœŸæ ¼å¼"""
    if not date_str:
        return datetime.now(timezone.utc).isoformat()
    
    formats = [
        "%a, %d %b %Y %H:%M:%S %z",
        "%a, %d %b %Y %H:%M:%S %Z",
        "%Y-%m-%dT%H:%M:%S%z",
        "%Y-%m-%dT%H:%M:%SZ",
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%dT%H:%M:%S.%f%z",
        "%Y-%m-%dT%H:%M:%S.%fZ",
    ]
    
    for fmt in formats:
        try:
            dt = datetime.strptime(date_str.strip(), fmt)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt.isoformat()
        except ValueError:
            continue
    
    # fallback: try dateutil
    try:
        from dateutil import parser as dateutil_parser
        dt = dateutil_parser.parse(date_str)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.isoformat()
    except:
        return datetime.now(timezone.utc).isoformat()

def classify_importance(title, summary):
    """æ ¹æ®å…³é”®è¯åˆ¤æ–­é‡è¦æ€§ 1-5"""
    text = f"{title} {summary}".lower()
    
    # é‡å¤§äº‹ä»¶å…³é”®è¯
    critical = ['war', 'invasion', 'nuclear', 'crash', 'crisis', 'emergency', 'breaking',
                 'æˆ˜äº‰', 'æ ¸', 'å´©ç›˜', 'å±æœº', 'ç´§æ€¥', 'çªå‘', 'é‡å¤§', 'åœ°éœ‡', 'æµ·å•¸']
    high = ['summit', 'sanctions', 'election', 'fed', 'interest rate', 'gdp', 'inflation',
            'trump', 'biden', 'xi jinping', 'putin',
            'å³°ä¼š', 'åˆ¶è£', 'é€‰ä¸¾', 'å¤®è¡Œ', 'åˆ©ç‡', 'GDP', 'é€šèƒ€', 'å…³ç¨', 'è´¸æ˜“æˆ˜',
            'ä¹ è¿‘å¹³', 'æ™®äº¬', 'ç‰¹æœ—æ™®', 'æ‹œç™»', 'ä¸¤ä¼š', 'æ”¿ç­–']
    medium = ['trade', 'market', 'stock', 'oil', 'gold', 'bitcoin',
              'è´¸æ˜“', 'å¸‚åœº', 'è‚¡å¸‚', 'çŸ³æ²¹', 'é»„é‡‘', 'æ¯”ç‰¹å¸', 'ç§‘æŠ€', 'èŠ¯ç‰‡']
    
    score = 3  # default medium
    for kw in critical:
        if kw in text:
            score = 5
            break
    if score < 5:
        for kw in high:
            if kw in text:
                score = 4
                break
    if score < 4:
        for kw in medium:
            if kw in text:
                score = 3
                break
    
    return score

def detect_region(title, summary, source_name):
    """æ£€æµ‹æ–°é—»æ¶‰åŠçš„åœ°åŒº"""
    text = f"{title} {summary} {source_name}".lower()
    
    regions = []
    region_map = {
        'ä¸­å›½': ['china', 'chinese', 'beijing', 'shanghai', 'ä¸­å›½', 'åŒ—äº¬', 'ä¸Šæµ·', 'ä¹ è¿‘å¹³', 'æ–°å', 'æ¾æ¹ƒ', 'è´¢æ–°'],
        'ç¾å›½': ['us', 'usa', 'america', 'washington', 'trump', 'biden', 'fed', 'wall street', 'ç¾å›½', 'åç››é¡¿', 'ç¾è”å‚¨'],
        'æ¬§æ´²': ['europe', 'eu', 'european', 'brussels', 'london', 'paris', 'berlin', 'æ¬§æ´²', 'æ¬§ç›Ÿ', 'è‹±å›½', 'æ³•å›½', 'å¾·å›½'],
        'ä¿„ç½—æ–¯': ['russia', 'russian', 'moscow', 'putin', 'kremlin', 'ä¿„ç½—æ–¯', 'è«æ–¯ç§‘', 'æ™®äº¬'],
        'ä¸­ä¸œ': ['middle east', 'israel', 'iran', 'saudi', 'gaza', 'syria', 'ä¸­ä¸œ', 'ä»¥è‰²åˆ—', 'ä¼Šæœ—', 'æ²™ç‰¹'],
        'äºšå¤ª': ['japan', 'korea', 'india', 'asean', 'asia', 'pacific', 'æ—¥æœ¬', 'éŸ©å›½', 'å°åº¦', 'ä¸œç›Ÿ', 'äºšæ´²'],
        'å…¨çƒ': ['global', 'world', 'international', 'un ', 'united nations', 'å…¨çƒ', 'ä¸–ç•Œ', 'è”åˆå›½'],
    }
    
    for region, keywords in region_map.items():
        for kw in keywords:
            if kw in text:
                regions.append(region)
                break
    
    return regions if regions else ['å…¶ä»–']

# ==================== ç¿»è¯‘ (SiliconFlow/DeepSeek AI) ====================

TRANSLATE_API_URL = os.environ.get('TRANSLATE_API_URL', 'https://api.siliconflow.cn/v1/chat/completions')
TRANSLATE_API_KEY = os.environ.get('TRANSLATE_API_KEY', '')
TRANSLATE_MODEL = os.environ.get('TRANSLATE_MODEL', 'deepseek-ai/DeepSeek-V3')

def ai_translate_batch(texts, batch_size=20):
    """ç”¨AIå¤§æ¨¡å‹æ‰¹é‡ç¿»è¯‘è‹±æ–‡ä¸ºä¸­æ–‡"""
    import requests as req
    
    if not TRANSLATE_API_KEY:
        print("  âš ï¸ æœªè®¾ç½® TRANSLATE_API_KEY ç¯å¢ƒå˜é‡ï¼Œè·³è¿‡ç¿»è¯‘")
        return texts
    
    results = list(texts)  # copy
    
    # ç­›é€‰å‡ºéœ€è¦ç¿»è¯‘çš„
    to_translate = []
    for i, t in enumerate(texts):
        if not t or not t.strip():
            continue
        cn_chars = len(re.findall(r'[\u4e00-\u9fff]', t))
        if cn_chars > len(t) * 0.3:
            continue  # å·²ç»æ˜¯ä¸­æ–‡
        to_translate.append((i, t[:300]))
    
    if not to_translate:
        return results
    
    # åˆ†æ‰¹ç¿»è¯‘
    for batch_start in range(0, len(to_translate), batch_size):
        batch = to_translate[batch_start:batch_start + batch_size]
        
        # æ„å»ºpromptï¼šç¼–å·åˆ—è¡¨æ–¹ä¾¿è§£æ
        lines = []
        for j, (idx, text) in enumerate(batch):
            lines.append(f"{j+1}. {text}")
        prompt_text = "\n".join(lines)
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»ç¿»è¯‘å™¨ã€‚å°†ä»¥ä¸‹ç¼–å·çš„è‹±æ–‡æ–°é—»æ ‡é¢˜/æ‘˜è¦ç¿»è¯‘æˆç®€æ´æµç•…çš„ä¸­æ–‡ã€‚
è§„åˆ™ï¼š
1. ä¿æŒç¼–å·æ ¼å¼ï¼Œæ¯è¡Œä¸€æ¡
2. åªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸åŠ è§£é‡Š
3. äººå/åœ°åç”¨é€šç”¨ä¸­æ–‡è¯‘å
4. ä¿æŒæ–°é—»æ ‡é¢˜çš„ç®€æ´é£æ ¼
5. ä¸“ä¸šæœ¯è¯­ç”¨å¸¸è§ä¸­æ–‡è¡¨è¾¾"""
        
        try:
            resp = req.post(TRANSLATE_API_URL, 
                headers={
                    'Content-Type': 'application/json',
                    'Authorization': f'Bearer {TRANSLATE_API_KEY}'
                },
                json={
                    'model': TRANSLATE_MODEL,
                    'messages': [
                        {'role': 'system', 'content': system_prompt},
                        {'role': 'user', 'content': prompt_text}
                    ],
                    'max_tokens': 2000,
                    'temperature': 0.3
                },
                timeout=30
            )
            resp.raise_for_status()
            data = resp.json()
            reply = data.get('choices', [{}])[0].get('message', {}).get('content', '')
            
            # è§£æç¿»è¯‘ç»“æœ
            translated_lines = reply.strip().split('\n')
            for line in translated_lines:
                line = line.strip()
                if not line:
                    continue
                # åŒ¹é… "1. ç¿»è¯‘å†…å®¹" æˆ– "1ã€ç¿»è¯‘å†…å®¹" æˆ– "1.ç¿»è¯‘å†…å®¹"
                m = re.match(r'^(\d+)\s*[.ã€ï¼]\s*(.+)', line)
                if m:
                    num = int(m.group(1)) - 1
                    translated = m.group(2).strip()
                    if 0 <= num < len(batch):
                        orig_idx = batch[num][0]
                        results[orig_idx] = translated
            
        except Exception as e:
            print(f"  âš ï¸ ç¿»è¯‘æ‰¹æ¬¡å¤±è´¥: {str(e)[:60]}")
        
        # é¿å…APIé™æµ
        if batch_start + batch_size < len(to_translate):
            time.sleep(1)
    
    return results

def translate_items(items):
    """ç¿»è¯‘æ‰€æœ‰è‹±æ–‡æ–°é—»çš„æ ‡é¢˜å’Œæ‘˜è¦"""
    en_items = [(i, item) for i, item in enumerate(items) if item.get('lang') == 'en']
    if not en_items:
        return items
    
    if not TRANSLATE_API_KEY:
        print(f"\nâš ï¸ è·³è¿‡ç¿»è¯‘ï¼ˆæœªè®¾ç½® TRANSLATE_API_KEYï¼‰")
        print(f"   ç”¨æ³•: TRANSLATE_API_KEY=sk-xxx python3 scripts/fetch_news.py")
        return items
    
    print(f"\nğŸŒ ç¿»è¯‘ {len(en_items)} æ¡è‹±æ–‡æ–°é—» (ä½¿ç”¨ {TRANSLATE_MODEL})...")
    
    titles = [item['title'] for _, item in en_items]
    summaries = [item.get('summary', '') for _, item in en_items]
    
    translated_titles = ai_translate_batch(titles, batch_size=25)
    translated_summaries = ai_translate_batch(summaries, batch_size=15)
    
    success = 0
    for j, (i, item) in enumerate(en_items):
        if translated_titles[j] and translated_titles[j] != item['title']:
            items[i]['title_original'] = item['title']
            items[i]['title'] = translated_titles[j]
            success += 1
        if translated_summaries[j] and translated_summaries[j] != item.get('summary', ''):
            items[i]['summary_original'] = item.get('summary', '')
            items[i]['summary'] = translated_summaries[j]
        items[i]['lang'] = 'zh-translated'
    
    print(f"  âœ… æˆåŠŸç¿»è¯‘ {success}/{len(en_items)} æ¡æ ‡é¢˜")
    return items

# ==================== RSS æŠ“å– ====================

def fetch_sina_finance(source, resp):
    """è§£ææ–°æµªè´¢ç»API"""
    items = []
    try:
        text = resp.text.strip()
        # Remove JSONP callback if present
        if text.startswith('('):
            text = text[1:-1]
        data = json.loads(text)
        for entry in (data.get('result', {}).get('data', []))[:20]:
            title = entry.get('title', '')
            if not title:
                continue
            items.append({
                'id': make_id(title, source['name']),
                'title': title,
                'summary': clean_html(entry.get('intro', '') or entry.get('summary', ''))[:300],
                'link': entry.get('url', ''),
                'source': source['name'],
                'source_icon': source['icon'],
                'category': source['category'],
                'lang': source['lang'],
                'image': entry.get('img', {}).get('u', '') if isinstance(entry.get('img'), dict) else '',
                'pub_date': parse_date(entry.get('ctime', '') or entry.get('createTime', '')),
                'fetch_time': datetime.now(timezone.utc).isoformat(),
                'importance': classify_importance(title, entry.get('intro', '')),
                'regions': detect_region(title, entry.get('intro', ''), source['name']),
                'priority': source['priority'],
            })
        print(f"  âœ… {source['name']}: {len(items)} æ¡")
    except Exception as e:
        print(f"  âŒ {source['name']}: {str(e)[:80]}")
    return items

# ==================== å›½å†…çƒ­æœå¹³å°æŠ“å– ====================

def fetch_douyin_hot():
    """æŠ“å–æŠ–éŸ³çƒ­æœæ¦œ"""
    import requests as req
    items = []
    name = 'æŠ–éŸ³çƒ­æœ'
    icon = 'ğŸµ'
    try:
        r = req.get('https://www.douyin.com/aweme/v1/web/hot/search/list/',
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://www.douyin.com/'
            }, timeout=15)
        r.raise_for_status()
        data = r.json()
        word_list = data.get('data', {}).get('word_list', [])
        
        for entry in word_list[:30]:
            title = entry.get('word', '').strip()
            if not title:
                continue
            hot_value = entry.get('hot_value', 0)
            event_time = entry.get('event_time', 0)
            
            # æ ¹æ®çƒ­åº¦åˆ¤æ–­é‡è¦æ€§
            importance = 3
            if hot_value > 10000000:
                importance = 5
            elif hot_value > 5000000:
                importance = 4
            
            pub_date = datetime.fromtimestamp(event_time, tz=timezone.utc).isoformat() if event_time else datetime.now(timezone.utc).isoformat()
            
            # æ™ºèƒ½åˆ†ç±»
            category = auto_classify_cn(title)
            
            items.append({
                'id': make_id(title, name),
                'title': title,
                'summary': f'ğŸ”¥ çƒ­åº¦: {hot_value:,}',
                'link': f'https://www.douyin.com/search/{title}',
                'source': name,
                'source_icon': icon,
                'category': category,
                'lang': 'zh',
                'image': '',
                'pub_date': pub_date,
                'fetch_time': datetime.now(timezone.utc).isoformat(),
                'importance': importance,
                'regions': detect_region(title, '', name),
                'priority': 1,
                'hot_value': hot_value,
            })
        print(f"  âœ… {name}: {len(items)} æ¡")
    except Exception as e:
        print(f"  âŒ {name}: {str(e)[:80]}")
    return items

def fetch_toutiao_hot():
    """æŠ“å–ä»Šæ—¥å¤´æ¡çƒ­æ¦œ"""
    import requests as req
    items = []
    name = 'ä»Šæ—¥å¤´æ¡'
    icon = 'ğŸ“±'
    try:
        r = req.get('https://www.toutiao.com/hot-event/hot-board/?origin=toutiao_pc',
            headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'},
            timeout=15)
        r.raise_for_status()
        data = r.json()
        entries = data.get('data', [])
        
        for i, entry in enumerate(entries[:30]):
            title = entry.get('Title', '').strip()
            if not title:
                continue
            hot_value = entry.get('HotValue', 0)
            try:
                hot_value = int(hot_value)
            except (ValueError, TypeError):
                hot_value = 0
            url = entry.get('Url', '')
            label = entry.get('Label', '')
            
            importance = 3
            if label == 'hot' or hot_value > 10000000:
                importance = 4
            if label == 'boom' or hot_value > 20000000:
                importance = 5
            if i < 3:
                importance = max(importance, 4)
            
            category = auto_classify_cn(title)
            
            items.append({
                'id': make_id(title, name),
                'title': title,
                'summary': f'ğŸ”¥ çƒ­åº¦: {hot_value:,}' + (f' Â· {label}' if label else ''),
                'link': url,
                'source': name,
                'source_icon': icon,
                'category': category,
                'lang': 'zh',
                'image': '',
                'pub_date': datetime.now(timezone.utc).isoformat(),
                'fetch_time': datetime.now(timezone.utc).isoformat(),
                'importance': importance,
                'regions': detect_region(title, '', name),
                'priority': 1,
                'hot_value': hot_value,
            })
        print(f"  âœ… {name}: {len(items)} æ¡")
    except Exception as e:
        print(f"  âŒ {name}: {str(e)[:80]}")
    return items

def fetch_36kr_newsflash():
    """æŠ“å–36æ°ªå¿«è®¯ï¼ˆè´¢ç»ç§‘æŠ€ï¼‰"""
    import requests as req
    items = []
    name = '36æ°ªå¿«è®¯'
    icon = 'ğŸ’¼'
    try:
        r = req.get('https://36kr.com/newsflashes',
            headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'},
            timeout=15)
        r.raise_for_status()
        
        m = re.search(r'window\.initialState\s*=\s*({.+?})\s*</script>', r.text, re.DOTALL)
        if not m:
            print(f"  âŒ {name}: æ— æ³•è§£æé¡µé¢æ•°æ®")
            return items
        
        raw = m.group(1)
        data = json.loads(raw)
        flash_list = data.get('newsflashCatalogData', {}).get('data', {}).get('newsflashList', {}).get('data', {}).get('itemList', [])
        
        for entry in flash_list[:20]:
            mat = entry.get('templateMaterial', {})
            title = mat.get('widgetTitle', '').strip()
            if not title:
                continue
            summary = clean_html(mat.get('widgetContent', ''))[:300]
            pub_time = mat.get('publishTime', 0)
            item_id = mat.get('itemId', '')
            
            pub_date = datetime.fromtimestamp(pub_time / 1000, tz=timezone.utc).isoformat() if pub_time > 1000000000 else datetime.now(timezone.utc).isoformat()
            
            # 36æ°ªä¸»è¦æ˜¯è´¢ç»ç§‘æŠ€
            category = auto_classify_cn(title + ' ' + summary)
            if category == 'æ—¶äº‹':
                category = 'è´¢ç»'  # 36æ°ªåå‘è´¢ç»
            
            items.append({
                'id': make_id(title, name),
                'title': title,
                'summary': summary,
                'link': f'https://36kr.com/newsflashes/{item_id}' if item_id else '',
                'source': name,
                'source_icon': icon,
                'category': category,
                'lang': 'zh',
                'image': '',
                'pub_date': pub_date,
                'fetch_time': datetime.now(timezone.utc).isoformat(),
                'importance': classify_importance(title, summary),
                'regions': detect_region(title, summary, name),
                'priority': 1,
            })
        print(f"  âœ… {name}: {len(items)} æ¡")
    except Exception as e:
        print(f"  âŒ {name}: {str(e)[:80]}")
    return items

def fetch_xiaohongshu_explore():
    """æŠ“å–å°çº¢ä¹¦æ¢ç´¢çƒ­é—¨å†…å®¹"""
    import requests as req
    items = []
    name = 'å°çº¢ä¹¦çƒ­é—¨'
    icon = 'ğŸ“•'
    try:
        r = req.get('https://www.xiaohongshu.com/explore',
            headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'},
            timeout=15)
        r.raise_for_status()
        
        m = re.search(r'window\.__INITIAL_STATE__\s*=\s*(.+?)</script>', r.text, re.DOTALL)
        if not m:
            print(f"  âŒ {name}: æ— æ³•è§£æé¡µé¢æ•°æ®")
            return items
        
        raw = m.group(1).strip().rstrip(';').replace('undefined', 'null')
        data = json.loads(raw)
        feeds = data.get('feed', {}).get('feeds', [])
        
        for entry in feeds[:20]:
            nc = entry.get('noteCard', entry)
            title = nc.get('displayTitle', '').strip()
            if not title:
                continue
            
            user = nc.get('user', {}).get('nickname', '')
            likes = nc.get('interactInfo', {}).get('likedCount', '')
            note_type = nc.get('type', 'normal')
            note_id = entry.get('id', '')
            
            category = auto_classify_cn(title)
            
            # æ ¹æ®ç‚¹èµä¼°ç®—é‡è¦æ€§
            importance = 3
            try:
                like_num = int(str(likes).replace('ä¸‡', '0000').replace('.', ''))
                if like_num > 50000:
                    importance = 5
                elif like_num > 10000:
                    importance = 4
            except:
                pass
            
            items.append({
                'id': make_id(title, name),
                'title': title,
                'summary': f'ğŸ‘¤ {user} Â· â¤ï¸ {likes}' + (f' Â· ğŸ¬ è§†é¢‘' if note_type == 'video' else ''),
                'link': f'https://www.xiaohongshu.com/explore/{note_id}' if note_id else '',
                'source': name,
                'source_icon': icon,
                'category': category,
                'lang': 'zh',
                'image': '',
                'pub_date': datetime.now(timezone.utc).isoformat(),
                'fetch_time': datetime.now(timezone.utc).isoformat(),
                'importance': importance,
                'regions': detect_region(title, '', name),
                'priority': 2,
            })
        print(f"  âœ… {name}: {len(items)} æ¡")
    except Exception as e:
        print(f"  âŒ {name}: {str(e)[:80]}")
    return items

def auto_classify_cn(text):
    """ä¸­æ–‡å†…å®¹æ™ºèƒ½åˆ†ç±»"""
    finance_kw = ['è‚¡', 'åŸºé‡‘', 'ç†è´¢', 'æŠ•èµ„', 'è´¢ç»', 'ä¸Šå¸‚', 'æ¶¨åœ', 'è·Œåœ', 'å¸‚å€¼', 
                  'Aè‚¡', 'æ¸¯è‚¡', 'ç¾è‚¡', 'å€ºåˆ¸', 'æœŸè´§', 'å¤–æ±‡', 'å¤®è¡Œ', 'åˆ©ç‡', 'é€šèƒ€',
                  'GDP', 'ç»æµ', 'é‡‘è', 'é“¶è¡Œ', 'ä¿é™©', 'è¯åˆ¸', 'èèµ„', 'èµ„æœ¬', 'ä¼°å€¼',
                  'è¥æ”¶', 'åˆ©æ¶¦', 'å›è´­', 'åˆ†çº¢', 'å‡æŒ', 'å¢æŒ', 'æ”¶è´­', 'å¹¶è´­', 'ä¸Šæ¶¨',
                  'ä¸‹è·Œ', 'ç‰›å¸‚', 'ç†Šå¸‚', 'äº¤æ˜“', 'èµ„é‡‘', 'æŒ‡æ•°', 'æ¿å—', 'æ¦‚å¿µè‚¡', 'å¸‚åœº',
                  'æ¶ˆè´¹', 'é›¶å”®', 'å‡ºå£', 'è¿›å£', 'ç¨', 'æ²¹ä»·', 'é‡‘ä»·', 'æ¯”ç‰¹å¸', 'æ•°å­—è´§å¸']
    politics_kw = ['æ”¿æ²»', 'æ”¿åºœ', 'å›½åŠ¡é™¢', 'å…¨å›½äººå¤§', 'æ”¿å', 'ä¸¤ä¼š', 'æ€»ä¹¦è®°', 'ä¸»å¸­',
                   'æ€»ç»Ÿ', 'é€‰ä¸¾', 'å¤–äº¤', 'åˆ¶è£', 'æ¡çº¦', 'æ³•æ¡ˆ', 'ç«‹æ³•', 'æ³•é™¢', 'æ”¿ç­–',
                   'æ”¹é©', 'ä¸€å¸¦ä¸€è·¯', 'å°æ¹¾', 'å—æµ·', 'å›½é˜²', 'å†›äº‹', 'éƒ¨é˜Ÿ']
    tech_kw = ['AI', 'äººå·¥æ™ºèƒ½', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', '5G', '6G', 'æœºå™¨äºº', 'è‡ªåŠ¨é©¾é©¶',
               'å¤§æ¨¡å‹', 'ç®—æ³•', 'ChatGPT', 'é‡å­', 'èˆªå¤©', 'ç«ç®­', 'å«æ˜Ÿ', 'ç§‘æŠ€',
               'äº’è”ç½‘', 'æ‰‹æœº', 'è‹¹æœ', 'åä¸º', 'ç‰¹æ–¯æ‹‰', 'æ–°èƒ½æº', 'ç”µæ± ', 'å…‰ä¼',
               'ç”Ÿç‰©', 'åŒ»è¯', 'ç–«è‹—', 'åŸºå› ', 'Kimi', 'DeepSeek', 'åƒé—®']
    intl_kw = ['ç¾å›½', 'ä¿„ç½—æ–¯', 'æ¬§æ´²', 'æ—¥æœ¬', 'éŸ©å›½', 'æœé²œ', 'ä¸­ä¸œ', 'ä»¥è‰²åˆ—',
               'ä¹Œå…‹å…°', 'åŒ—çº¦', 'è”åˆå›½', 'å›½é™…', 'å…¨çƒ', 'æµ·å¤–', 'å‡ºæµ·']
    
    for kw in finance_kw:
        if kw in text:
            return 'è´¢ç»'
    for kw in politics_kw:
        if kw in text:
            return 'æ”¿æ²»'
    for kw in tech_kw:
        if kw in text:
            return 'ç§‘æŠ€'
    for kw in intl_kw:
        if kw in text:
            return 'å›½é™…'
    return 'æ—¶äº‹'

# ==================== RSS æŠ“å– ====================

def fetch_single_rss(source):
    """æŠ“å–å•ä¸ªRSSæº"""
    import feedparser
    import requests
    
    items = []
    try:
        # ä½¿ç”¨requestsè·å–å†…å®¹ï¼ˆæ›´å¥½çš„è¶…æ—¶æ§åˆ¶ï¼‰
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        resp = requests.get(source['url'], headers=headers, timeout=15)
        resp.raise_for_status()
        
        # æ–°æµªAPIç‰¹æ®Šå¤„ç†
        if source.get('type') == 'sina_api':
            return fetch_sina_finance(source, resp)
        
        feed = feedparser.parse(resp.content)
        
        for entry in feed.entries[:20]:  # æ¯ä¸ªæºæœ€å¤šå–20æ¡
            title = clean_html(entry.get('title', ''))
            if not title:
                continue
            
            summary = clean_html(
                entry.get('summary', '') or 
                entry.get('description', '') or 
                entry.get('content', [{}])[0].get('value', '') if entry.get('content') else ''
            )
            
            link = entry.get('link', '')
            pub_date = entry.get('published', '') or entry.get('updated', '')
            
            # æå–å›¾ç‰‡
            image = ''
            if entry.get('media_content'):
                image = entry['media_content'][0].get('url', '')
            elif entry.get('media_thumbnail'):
                image = entry['media_thumbnail'][0].get('url', '')
            elif entry.get('enclosures'):
                for enc in entry['enclosures']:
                    if 'image' in enc.get('type', ''):
                        image = enc.get('href', '')
                        break
            
            importance = classify_importance(title, summary)
            regions = detect_region(title, summary, source['name'])
            
            item = {
                'id': make_id(title, source['name']),
                'title': title,
                'summary': summary[:300],
                'link': link,
                'source': source['name'],
                'source_icon': source['icon'],
                'category': source['category'],
                'lang': source['lang'],
                'image': image,
                'pub_date': parse_date(pub_date),
                'fetch_time': datetime.now(timezone.utc).isoformat(),
                'importance': importance,
                'regions': regions,
                'priority': source['priority'],
            }
            items.append(item)
        
        print(f"  âœ… {source['name']}: {len(items)} æ¡")
    except Exception as e:
        print(f"  âŒ {source['name']}: {str(e)[:80]}")
    
    return items

def fetch_all_news():
    """å¹¶å‘æŠ“å–æ‰€æœ‰RSSæº + å›½å†…çƒ­æœå¹³å°"""
    cn_fetchers = [
        ('æŠ–éŸ³çƒ­æœ', fetch_douyin_hot),
        ('ä»Šæ—¥å¤´æ¡', fetch_toutiao_hot),
        ('36æ°ªå¿«è®¯', fetch_36kr_newsflash),
        ('å°çº¢ä¹¦çƒ­é—¨', fetch_xiaohongshu_explore),
    ]
    total_sources = len(RSS_SOURCES) + len(cn_fetchers)
    
    print(f"\nğŸŒ å¼€å§‹æŠ“å–å…¨çƒæ–°é—» [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]")
    print(f"   å…± {total_sources} ä¸ªæº ({len(RSS_SOURCES)} RSS + {len(cn_fetchers)} å›½å†…çƒ­æœ)\n")
    
    all_items = []
    
    with ThreadPoolExecutor(max_workers=8) as executor:
        # RSS sources
        futures = {executor.submit(fetch_single_rss, src): src['name'] for src in RSS_SOURCES}
        # å›½å†…çƒ­æœå¹³å°
        for name, func in cn_fetchers:
            futures[executor.submit(func)] = name
        
        for future in as_completed(futures):
            items = future.result()
            all_items.extend(items)
    
    # å»é‡ï¼ˆæŒ‰æ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰
    seen_titles = set()
    unique_items = []
    for item in all_items:
        # ç®€å•å»é‡ï¼šæ ‡é¢˜å‰30å­—ç¬¦
        title_key = re.sub(r'\s+', '', item['title'][:30]).lower()
        if title_key not in seen_titles:
            seen_titles.add(title_key)
            unique_items.append(item)
    
    # æ’åºï¼šé‡è¦æ€§ Ã— ä¼˜å…ˆçº§ Ã— æ—¶é—´
    def sort_key(item):
        try:
            dt = datetime.fromisoformat(item['pub_date'].replace('Z', '+00:00'))
            hours_ago = (datetime.now(timezone.utc) - dt).total_seconds() / 3600
        except:
            hours_ago = 24
        
        # ç»¼åˆåˆ†æ•°ï¼šé‡è¦æ€§é«˜+æºä¼˜å…ˆçº§é«˜+è¶Šæ–°è¶Šå¥½
        return -(item['importance'] * 10 + (3 - item['priority']) * 5 - hours_ago * 0.5)
    
    unique_items.sort(key=sort_key)
    unique_items = unique_items[:MAX_NEWS]
    
    # ç¿»è¯‘è‹±æ–‡æ–°é—»
    unique_items = translate_items(unique_items)
    
    print(f"\nğŸ“Š æ±‡æ€»: æŠ“å– {len(all_items)} æ¡, å»é‡å {len(unique_items)} æ¡")
    
    # ç»Ÿè®¡
    cats = {}
    for item in unique_items:
        cats[item['category']] = cats.get(item['category'], 0) + 1
    for cat, count in sorted(cats.items(), key=lambda x: -x[1]):
        print(f"   {cat}: {count} æ¡")
    
    return unique_items

def save_news(items):
    """ä¿å­˜ä¸ºJSON"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    # åˆå¹¶å†å²æ•°æ®ï¼ˆä¿ç•™æœ€è¿‘çš„ï¼‰
    existing = []
    if OUTPUT_FILE.exists():
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                existing = data.get('items', [])
        except:
            pass
    
    # åˆå¹¶å»é‡
    existing_ids = {item['id'] for item in items}
    for old_item in existing:
        if old_item['id'] not in existing_ids:
            items.append(old_item)
            existing_ids.add(old_item['id'])
    
    # åªä¿ç•™7å¤©å†…çš„
    cutoff = (datetime.now(timezone.utc) - timedelta(days=7)).isoformat()
    items = [i for i in items if i.get('pub_date', '') >= cutoff or i.get('fetch_time', '') >= cutoff]
    items = items[:MAX_NEWS]
    
    output = {
        'last_update': datetime.now(timezone.utc).isoformat(),
        'total': len(items),
        'sources': len(RSS_SOURCES),
        'items': items
    }
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ å·²ä¿å­˜ {len(items)} æ¡æ–°é—»åˆ° {OUTPUT_FILE}")

# ==================== ä¸»ç¨‹åº ====================

if __name__ == '__main__':
    import argparse
    import sys
    parser = argparse.ArgumentParser(description='å…¨çƒæ—¶äº‹æ–°é—»èšåˆå™¨')
    parser.add_argument('--loop', type=int, default=0, help='å¾ªç¯æŠ“å–é—´éš”(åˆ†é’Ÿ), 0=åªæ‰§è¡Œä¸€æ¬¡')
    parser.add_argument('--api-key', type=str, default='', help='AI API Key (ç”¨äºç¿»è¯‘è‹±æ–‡æ–°é—»)')
    parser.add_argument('--api-url', type=str, default='', help='AI API URL')
    parser.add_argument('--model', type=str, default='', help='AIæ¨¡å‹åç§°')
    args = parser.parse_args()
    
    # è®¾ç½®ç¿»è¯‘API
    _mod = sys.modules[__name__]
    if args.api_key:
        _mod.TRANSLATE_API_KEY = args.api_key
    if args.api_url:
        _mod.TRANSLATE_API_URL = args.api_url
    if args.model:
        _mod.TRANSLATE_MODEL = args.model
    
    # å®‰è£…ä¾èµ–
    try:
        import feedparser
    except ImportError:
        print("ğŸ“¦ å®‰è£… feedparser...")
        os.system("pip3 install feedparser")
        import feedparser
    
    try:
        import requests
    except ImportError:
        print("ğŸ“¦ å®‰è£… requests...")
        os.system("pip3 install requests")
        import requests
    
    if args.loop > 0:
        print(f"ğŸ”„ å¾ªç¯æ¨¡å¼: æ¯ {args.loop} åˆ†é’ŸæŠ“å–ä¸€æ¬¡ (Ctrl+C é€€å‡º)")
        while True:
            try:
                items = fetch_all_news()
                save_news(items)
                print(f"\nâ° ä¸‹æ¬¡æŠ“å–: {(datetime.now() + timedelta(minutes=args.loop)).strftime('%H:%M:%S')}")
                time.sleep(args.loop * 60)
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å·²åœæ­¢")
                break
            except Exception as e:
                print(f"\nâŒ å‡ºé”™: {e}")
                traceback.print_exc()
                time.sleep(60)
    else:
        items = fetch_all_news()
        save_news(items)
        print("\nâœ… å®Œæˆ!")
