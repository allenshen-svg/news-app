#!/usr/bin/env python3
"""
å…¨çƒæ—¶äº‹æ”¿ç»æ–°é—»èšåˆå™¨
ä»å¤šä¸ªRSSæºå’ŒAPIæŠ“å–æ–°é—»ï¼Œåˆ†ç±»æ•´ç†åè¾“å‡ºJSON
"""

import json
import os
import re
import hashlib
import time
import traceback
from datetime import datetime, timezone, timedelta
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==================== é…ç½® ====================
DATA_DIR = Path(__file__).parent.parent / "data"
OUTPUT_FILE = DATA_DIR / "news.json"
MAX_NEWS = 200  # æœ€å¤šä¿ç•™æ¡æ•°

# RSS æºé…ç½®
RSS_SOURCES = [
    # ---- å›½é™…ç»¼åˆ ----
    {
        "name": "Reuters World",
        "url": "https://feeds.reuters.com/Reuters/worldNews",
        "category": "å›½é™…",
        "lang": "en",
        "icon": "ğŸŒ",
        "priority": 1
    },
    {
        "name": "BBC World",
        "url": "http://feeds.bbci.co.uk/news/world/rss.xml",
        "category": "å›½é™…",
        "lang": "en",
        "icon": "ğŸ‡¬ğŸ‡§",
        "priority": 1
    },
    {
        "name": "Al Jazeera",
        "url": "https://www.aljazeera.com/xml/rss/all.xml",
        "category": "å›½é™…",
        "lang": "en",
        "icon": "ğŸŒ",
        "priority": 2
    },
    {
        "name": "NPR World",
        "url": "https://feeds.npr.org/1004/rss.xml",
        "category": "å›½é™…",
        "lang": "en",
        "icon": "ğŸ“»",
        "priority": 2
    },
    # ---- è´¢ç» ----
    {
        "name": "CNBC Top",
        "url": "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=100003114",
        "category": "è´¢ç»",
        "lang": "en",
        "icon": "ğŸ’¹",
        "priority": 1
    },
    {
        "name": "Bloomberg",
        "url": "https://feeds.bloomberg.com/markets/news.rss",
        "category": "è´¢ç»",
        "lang": "en",
        "icon": "ğŸ“Š",
        "priority": 1
    },
    {
        "name": "FT World",
        "url": "https://www.ft.com/rss/home",
        "category": "è´¢ç»",
        "lang": "en",
        "icon": "ğŸ‡¬ğŸ‡§",
        "priority": 2
    },
    # ---- æ”¿æ²» ----
    {
        "name": "BBC Politics",
        "url": "http://feeds.bbci.co.uk/news/politics/rss.xml",
        "category": "æ”¿æ²»",
        "lang": "en",
        "icon": "ğŸ›ï¸",
        "priority": 2
    },
    {
        "name": "Reuters Politics",
        "url": "https://feeds.reuters.com/Reuters/PoliticsNews",
        "category": "æ”¿æ²»",
        "lang": "en",
        "icon": "ğŸ—³ï¸",
        "priority": 1
    },
    # ---- ç§‘æŠ€ ----
    {
        "name": "Reuters Tech",
        "url": "https://feeds.reuters.com/reuters/technologyNews",
        "category": "ç§‘æŠ€",
        "lang": "en",
        "icon": "ğŸ’»",
        "priority": 2
    },
    {
        "name": "TechCrunch",
        "url": "https://techcrunch.com/feed/",
        "category": "ç§‘æŠ€",
        "lang": "en",
        "icon": "ğŸš€",
        "priority": 2
    },
    # ---- ä¸­æ–‡æº ----
    {
        "name": "æ–°æµªè´¢ç»",
        "url": "https://feed.mix.sina.com.cn/api/roll/get?pageid=153&lid=2516&k=&num=20&page=1&r=0.1&callback=",
        "category": "è´¢ç»",
        "lang": "zh",
        "icon": "ğŸ“ˆ",
        "priority": 1,
        "type": "sina_api"
    },
    {
        "name": "BBCä¸­æ–‡",
        "url": "https://feeds.bbci.co.uk/zhongwen/simp/rss.xml",
        "category": "å›½é™…",
        "lang": "zh",
        "icon": "ğŸ‡¬ğŸ‡§",
        "priority": 1
    },
    {
        "name": "çº½çº¦æ—¶æŠ¥ä¸­æ–‡",
        "url": "https://cn.nytimes.com/rss/",
        "category": "å›½é™…",
        "lang": "zh",
        "icon": "ğŸ‡ºğŸ‡¸",
        "priority": 1
    },
    {
        "name": "éŸ©è”ç¤¾ä¸­æ–‡",
        "url": "https://cn.yna.co.kr/RSS/news.xml",
        "category": "å›½é™…",
        "lang": "zh",
        "icon": "ğŸ‡°ğŸ‡·",
        "priority": 2
    },
    {
        "name": "DWå¾·å›½ä¹‹å£°",
        "url": "https://rss.dw.com/xml/rss-chi-all",
        "category": "å›½é™…",
        "lang": "zh",
        "icon": "ğŸ‡©ğŸ‡ª",
        "priority": 2
    },
    {
        "name": "Nikkei Asia",
        "url": "https://asia.nikkei.com/rss",
        "category": "è´¢ç»",
        "lang": "en",
        "icon": "ğŸ‡¯ğŸ‡µ",
        "priority": 2
    },
    {
        "name": "The Guardian World",
        "url": "https://www.theguardian.com/world/rss",
        "category": "å›½é™…",
        "lang": "en",
        "icon": "ğŸ‡¬ğŸ‡§",
        "priority": 2
    },
    {
        "name": "WSJ Markets",
        "url": "https://feeds.content.dowjones.io/public/rss/mw_topstories",
        "category": "è´¢ç»",
        "lang": "en",
        "icon": "ğŸ“Š",
        "priority": 1
    },
]

# ==================== å·¥å…·å‡½æ•° ====================

def clean_html(text):
    """å»é™¤HTMLæ ‡ç­¾"""
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'&[a-zA-Z]+;', ' ', text)
    text = re.sub(r'&#\d+;', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text[:500]  # é™åˆ¶é•¿åº¦

def make_id(title, source):
    """ç”Ÿæˆå”¯ä¸€ID"""
    raw = f"{title}_{source}"
    return hashlib.md5(raw.encode()).hexdigest()[:12]

def parse_date(date_str):
    """è§£æå„ç§æ—¥æœŸæ ¼å¼"""
    if not date_str:
        return datetime.now(timezone.utc).isoformat()
    
    formats = [
        "%a, %d %b %Y %H:%M:%S %z",
        "%a, %d %b %Y %H:%M:%S %Z",
        "%Y-%m-%dT%H:%M:%S%z",
        "%Y-%m-%dT%H:%M:%SZ",
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%dT%H:%M:%S.%f%z",
        "%Y-%m-%dT%H:%M:%S.%fZ",
    ]
    
    for fmt in formats:
        try:
            dt = datetime.strptime(date_str.strip(), fmt)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt.isoformat()
        except ValueError:
            continue
    
    # fallback: try dateutil
    try:
        from dateutil import parser as dateutil_parser
        dt = dateutil_parser.parse(date_str)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.isoformat()
    except:
        return datetime.now(timezone.utc).isoformat()

def classify_importance(title, summary):
    """æ ¹æ®å…³é”®è¯åˆ¤æ–­é‡è¦æ€§ 1-5"""
    text = f"{title} {summary}".lower()
    
    # é‡å¤§äº‹ä»¶å…³é”®è¯
    critical = ['war', 'invasion', 'nuclear', 'crash', 'crisis', 'emergency', 'breaking',
                 'æˆ˜äº‰', 'æ ¸', 'å´©ç›˜', 'å±æœº', 'ç´§æ€¥', 'çªå‘', 'é‡å¤§', 'åœ°éœ‡', 'æµ·å•¸']
    high = ['summit', 'sanctions', 'election', 'fed', 'interest rate', 'gdp', 'inflation',
            'trump', 'biden', 'xi jinping', 'putin',
            'å³°ä¼š', 'åˆ¶è£', 'é€‰ä¸¾', 'å¤®è¡Œ', 'åˆ©ç‡', 'GDP', 'é€šèƒ€', 'å…³ç¨', 'è´¸æ˜“æˆ˜',
            'ä¹ è¿‘å¹³', 'æ™®äº¬', 'ç‰¹æœ—æ™®', 'æ‹œç™»', 'ä¸¤ä¼š', 'æ”¿ç­–']
    medium = ['trade', 'market', 'stock', 'oil', 'gold', 'bitcoin',
              'è´¸æ˜“', 'å¸‚åœº', 'è‚¡å¸‚', 'çŸ³æ²¹', 'é»„é‡‘', 'æ¯”ç‰¹å¸', 'ç§‘æŠ€', 'èŠ¯ç‰‡']
    
    score = 3  # default medium
    for kw in critical:
        if kw in text:
            score = 5
            break
    if score < 5:
        for kw in high:
            if kw in text:
                score = 4
                break
    if score < 4:
        for kw in medium:
            if kw in text:
                score = 3
                break
    
    return score

def detect_region(title, summary, source_name):
    """æ£€æµ‹æ–°é—»æ¶‰åŠçš„åœ°åŒº"""
    text = f"{title} {summary} {source_name}".lower()
    
    regions = []
    region_map = {
        'ä¸­å›½': ['china', 'chinese', 'beijing', 'shanghai', 'ä¸­å›½', 'åŒ—äº¬', 'ä¸Šæµ·', 'ä¹ è¿‘å¹³', 'æ–°å', 'æ¾æ¹ƒ', 'è´¢æ–°'],
        'ç¾å›½': ['us', 'usa', 'america', 'washington', 'trump', 'biden', 'fed', 'wall street', 'ç¾å›½', 'åç››é¡¿', 'ç¾è”å‚¨'],
        'æ¬§æ´²': ['europe', 'eu', 'european', 'brussels', 'london', 'paris', 'berlin', 'æ¬§æ´²', 'æ¬§ç›Ÿ', 'è‹±å›½', 'æ³•å›½', 'å¾·å›½'],
        'ä¿„ç½—æ–¯': ['russia', 'russian', 'moscow', 'putin', 'kremlin', 'ä¿„ç½—æ–¯', 'è«æ–¯ç§‘', 'æ™®äº¬'],
        'ä¸­ä¸œ': ['middle east', 'israel', 'iran', 'saudi', 'gaza', 'syria', 'ä¸­ä¸œ', 'ä»¥è‰²åˆ—', 'ä¼Šæœ—', 'æ²™ç‰¹'],
        'äºšå¤ª': ['japan', 'korea', 'india', 'asean', 'asia', 'pacific', 'æ—¥æœ¬', 'éŸ©å›½', 'å°åº¦', 'ä¸œç›Ÿ', 'äºšæ´²'],
        'å…¨çƒ': ['global', 'world', 'international', 'un ', 'united nations', 'å…¨çƒ', 'ä¸–ç•Œ', 'è”åˆå›½'],
    }
    
    for region, keywords in region_map.items():
        for kw in keywords:
            if kw in text:
                regions.append(region)
                break
    
    return regions if regions else ['å…¶ä»–']

# ==================== RSS æŠ“å– ====================

def fetch_sina_finance(source, resp):
    """è§£ææ–°æµªè´¢ç»API"""
    items = []
    try:
        text = resp.text.strip()
        # Remove JSONP callback if present
        if text.startswith('('):
            text = text[1:-1]
        data = json.loads(text)
        for entry in (data.get('result', {}).get('data', []))[:20]:
            title = entry.get('title', '')
            if not title:
                continue
            items.append({
                'id': make_id(title, source['name']),
                'title': title,
                'summary': clean_html(entry.get('intro', '') or entry.get('summary', ''))[:300],
                'link': entry.get('url', ''),
                'source': source['name'],
                'source_icon': source['icon'],
                'category': source['category'],
                'lang': source['lang'],
                'image': entry.get('img', {}).get('u', '') if isinstance(entry.get('img'), dict) else '',
                'pub_date': parse_date(entry.get('ctime', '') or entry.get('createTime', '')),
                'fetch_time': datetime.now(timezone.utc).isoformat(),
                'importance': classify_importance(title, entry.get('intro', '')),
                'regions': detect_region(title, entry.get('intro', ''), source['name']),
                'priority': source['priority'],
            })
        print(f"  âœ… {source['name']}: {len(items)} æ¡")
    except Exception as e:
        print(f"  âŒ {source['name']}: {str(e)[:80]}")
    return items

def fetch_single_rss(source):
    """æŠ“å–å•ä¸ªRSSæº"""
    import feedparser
    import requests
    
    items = []
    try:
        # ä½¿ç”¨requestsè·å–å†…å®¹ï¼ˆæ›´å¥½çš„è¶…æ—¶æ§åˆ¶ï¼‰
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        resp = requests.get(source['url'], headers=headers, timeout=15)
        resp.raise_for_status()
        
        # æ–°æµªAPIç‰¹æ®Šå¤„ç†
        if source.get('type') == 'sina_api':
            return fetch_sina_finance(source, resp)
        
        feed = feedparser.parse(resp.content)
        
        for entry in feed.entries[:20]:  # æ¯ä¸ªæºæœ€å¤šå–20æ¡
            title = clean_html(entry.get('title', ''))
            if not title:
                continue
            
            summary = clean_html(
                entry.get('summary', '') or 
                entry.get('description', '') or 
                entry.get('content', [{}])[0].get('value', '') if entry.get('content') else ''
            )
            
            link = entry.get('link', '')
            pub_date = entry.get('published', '') or entry.get('updated', '')
            
            # æå–å›¾ç‰‡
            image = ''
            if entry.get('media_content'):
                image = entry['media_content'][0].get('url', '')
            elif entry.get('media_thumbnail'):
                image = entry['media_thumbnail'][0].get('url', '')
            elif entry.get('enclosures'):
                for enc in entry['enclosures']:
                    if 'image' in enc.get('type', ''):
                        image = enc.get('href', '')
                        break
            
            importance = classify_importance(title, summary)
            regions = detect_region(title, summary, source['name'])
            
            item = {
                'id': make_id(title, source['name']),
                'title': title,
                'summary': summary[:300],
                'link': link,
                'source': source['name'],
                'source_icon': source['icon'],
                'category': source['category'],
                'lang': source['lang'],
                'image': image,
                'pub_date': parse_date(pub_date),
                'fetch_time': datetime.now(timezone.utc).isoformat(),
                'importance': importance,
                'regions': regions,
                'priority': source['priority'],
            }
            items.append(item)
        
        print(f"  âœ… {source['name']}: {len(items)} æ¡")
    except Exception as e:
        print(f"  âŒ {source['name']}: {str(e)[:80]}")
    
    return items

def fetch_all_news():
    """å¹¶å‘æŠ“å–æ‰€æœ‰RSSæº"""
    print(f"\nğŸŒ å¼€å§‹æŠ“å–å…¨çƒæ–°é—» [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]")
    print(f"   å…± {len(RSS_SOURCES)} ä¸ªæº\n")
    
    all_items = []
    
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = {executor.submit(fetch_single_rss, src): src for src in RSS_SOURCES}
        for future in as_completed(futures):
            items = future.result()
            all_items.extend(items)
    
    # å»é‡ï¼ˆæŒ‰æ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰
    seen_titles = set()
    unique_items = []
    for item in all_items:
        # ç®€å•å»é‡ï¼šæ ‡é¢˜å‰30å­—ç¬¦
        title_key = re.sub(r'\s+', '', item['title'][:30]).lower()
        if title_key not in seen_titles:
            seen_titles.add(title_key)
            unique_items.append(item)
    
    # æ’åºï¼šé‡è¦æ€§ Ã— ä¼˜å…ˆçº§ Ã— æ—¶é—´
    def sort_key(item):
        try:
            dt = datetime.fromisoformat(item['pub_date'].replace('Z', '+00:00'))
            hours_ago = (datetime.now(timezone.utc) - dt).total_seconds() / 3600
        except:
            hours_ago = 24
        
        # ç»¼åˆåˆ†æ•°ï¼šé‡è¦æ€§é«˜+æºä¼˜å…ˆçº§é«˜+è¶Šæ–°è¶Šå¥½
        return -(item['importance'] * 10 + (3 - item['priority']) * 5 - hours_ago * 0.5)
    
    unique_items.sort(key=sort_key)
    unique_items = unique_items[:MAX_NEWS]
    
    print(f"\nğŸ“Š æ±‡æ€»: æŠ“å– {len(all_items)} æ¡, å»é‡å {len(unique_items)} æ¡")
    
    # ç»Ÿè®¡
    cats = {}
    for item in unique_items:
        cats[item['category']] = cats.get(item['category'], 0) + 1
    for cat, count in sorted(cats.items(), key=lambda x: -x[1]):
        print(f"   {cat}: {count} æ¡")
    
    return unique_items

def save_news(items):
    """ä¿å­˜ä¸ºJSON"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    # åˆå¹¶å†å²æ•°æ®ï¼ˆä¿ç•™æœ€è¿‘çš„ï¼‰
    existing = []
    if OUTPUT_FILE.exists():
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                existing = data.get('items', [])
        except:
            pass
    
    # åˆå¹¶å»é‡
    existing_ids = {item['id'] for item in items}
    for old_item in existing:
        if old_item['id'] not in existing_ids:
            items.append(old_item)
            existing_ids.add(old_item['id'])
    
    # åªä¿ç•™7å¤©å†…çš„
    cutoff = (datetime.now(timezone.utc) - timedelta(days=7)).isoformat()
    items = [i for i in items if i.get('pub_date', '') >= cutoff or i.get('fetch_time', '') >= cutoff]
    items = items[:MAX_NEWS]
    
    output = {
        'last_update': datetime.now(timezone.utc).isoformat(),
        'total': len(items),
        'sources': len(RSS_SOURCES),
        'items': items
    }
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ å·²ä¿å­˜ {len(items)} æ¡æ–°é—»åˆ° {OUTPUT_FILE}")

# ==================== ä¸»ç¨‹åº ====================

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='å…¨çƒæ—¶äº‹æ–°é—»èšåˆå™¨')
    parser.add_argument('--loop', type=int, default=0, help='å¾ªç¯æŠ“å–é—´éš”(åˆ†é’Ÿ), 0=åªæ‰§è¡Œä¸€æ¬¡')
    args = parser.parse_args()
    
    # å®‰è£…ä¾èµ–
    try:
        import feedparser
    except ImportError:
        print("ğŸ“¦ å®‰è£… feedparser...")
        os.system("pip3 install feedparser")
        import feedparser
    
    try:
        import requests
    except ImportError:
        print("ğŸ“¦ å®‰è£… requests...")
        os.system("pip3 install requests")
        import requests
    
    if args.loop > 0:
        print(f"ğŸ”„ å¾ªç¯æ¨¡å¼: æ¯ {args.loop} åˆ†é’ŸæŠ“å–ä¸€æ¬¡ (Ctrl+C é€€å‡º)")
        while True:
            try:
                items = fetch_all_news()
                save_news(items)
                print(f"\nâ° ä¸‹æ¬¡æŠ“å–: {(datetime.now() + timedelta(minutes=args.loop)).strftime('%H:%M:%S')}")
                time.sleep(args.loop * 60)
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å·²åœæ­¢")
                break
            except Exception as e:
                print(f"\nâŒ å‡ºé”™: {e}")
                traceback.print_exc()
                time.sleep(60)
    else:
        items = fetch_all_news()
        save_news(items)
        print("\nâœ… å®Œæˆ!")
