#!/usr/bin/env python3
"""
æ¨¡å—äºŒ + æ¨¡å—ä¸‰ï¼šNLPæ–‡æœ¬å¤„ç†æµæ°´çº¿ & çƒ­åº¦è¯„ä¼°ä¸çªå‘æ£€æµ‹ç®—æ³•
============================================================

ã€NLP æµæ°´çº¿ã€‘
Raw Text â†’ æ¸…æ´— â†’ åˆ†è¯(jieba) â†’ åœç”¨è¯è¿‡æ»¤ â†’ å®ä½“è¯†åˆ«
â†’ TF-IDF å…³é”®è¯æå– â†’ TextRank çŸ­è¯­æå– â†’ æ–°è¯å‘ç°

ã€çƒ­åº¦è¯„ä¼°æ•°å­¦æ¨¡å‹ã€‘

1. å®æ—¶çƒ­åŠ›å€¼å…¬å¼ (Heat Score):
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ H(t) = Î±Â·F(t)Â·e^{-Î»(t_now - t_last)}                    â”‚
   â”‚      + Î²Â·A(t)                                             â”‚
   â”‚      + Î³Â·S(t)                                             â”‚
   â”‚      + Î´Â·E(t)                                             â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ F(t) = è¯é¢‘ (æ»‘åŠ¨çª—å£å†…å‡ºç°æ¬¡æ•°)                            â”‚
   â”‚ e^-Î»Î”t = ç‰›é¡¿å†·å´è¡°å‡ (åŠè¡°æœŸ = ln2/Î»)                    â”‚
   â”‚ A(t) = åŠ é€Ÿåº¦ = dF/dt (é¢‘ç‡å˜åŒ–ç‡, è¶Šå¿«è¶Šçƒ­)               â”‚
   â”‚ S(t) = æ¥æºå¤šæ ·æ€§ (è·¨å¹³å°å‡ºç° â†’ æ›´å¯èƒ½æ˜¯çœŸçƒ­ç‚¹)             â”‚
   â”‚ E(t) = äº’åŠ¨é‡å½’ä¸€ (ç‚¹èµ+è¯„è®º+åˆ†äº«åŠ æƒ)                     â”‚
   â”‚ Î±=0.4, Î²=0.3, Î³=0.2, Î´=0.1 (å¯è°ƒæƒé‡)                    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. çªå‘æ£€æµ‹ (Burst Detection):
   - Z-Score å¼‚å¸¸æ£€æµ‹ï¼šz = (x_t - Î¼) / Ïƒï¼Œå½“ z > 2.5 æ ‡è®°ä¸º burst
   - MACD è¶‹åŠ¿åŠ¨é‡ï¼š
     * çŸ­æœŸ EMA(12çª—å£) vs é•¿æœŸ EMA(26çª—å£)
     * MACD = Short_EMA - Long_EMA
     * Signal = EMA(MACD, 9)
     * MACD ä¸Šç©¿ Signal â†’ è¶‹åŠ¿å¯åŠ¨

3. çƒ­åº¦è¡°å‡ (Newton's Cooling Law):
   T(t) = T_env + (T_0 - T_env) Â· e^{-Î»t}
   â†’ ç®€åŒ–ä¸º: heat(t) = heat_peak Â· e^{-Î»Â·hours_since_peak}
   â†’ Î» = ln(2) / half_life_hours (é»˜è®¤åŠè¡°æœŸ4å°æ—¶)
"""

import json
import math
import re
import time
import hashlib
import logging
from collections import Counter, defaultdict
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Set
from dataclasses import dataclass, field, asdict

logger = logging.getLogger('trend_engine')

# ==================== é…ç½® ====================
# çƒ­åº¦è¯„ä¼°æƒé‡
ALPHA = 0.4   # è¯é¢‘æƒé‡
BETA  = 0.3   # åŠ é€Ÿåº¦æƒé‡
GAMMA = 0.2   # æ¥æºå¤šæ ·æ€§æƒé‡
DELTA = 0.1   # äº’åŠ¨é‡æƒé‡

# ç‰›é¡¿å†·å´å‚æ•°
HALF_LIFE_HOURS = 4.0   # åŠè¡°æœŸï¼ˆå°æ—¶ï¼‰
LAMBDA_DECAY = math.log(2) / HALF_LIFE_HOURS

# çªå‘æ£€æµ‹å‚æ•°
BURST_Z_THRESHOLD = 2.5    # Z-Score è¶…è¿‡æ­¤å€¼è§†ä¸ºçªå‘
MACD_SHORT_PERIOD = 12     # MACD çŸ­æœŸ EMA çª—å£
MACD_LONG_PERIOD = 26      # MACD é•¿æœŸ EMA çª—å£
MACD_SIGNAL_PERIOD = 9     # MACD ä¿¡å·çº¿ EMA çª—å£

# æ—¶é—´çª—å£é…ç½®
WINDOW_SIZE_MINUTES = 10   # æ¯ä¸ªç»Ÿè®¡çª—å£å¤§å°
HISTORY_WINDOWS = 144      # ä¿ç•™å†å²çª—å£æ•° (144 Ã— 10min = 24h)

# æ•°æ®è·¯å¾„
DATA_DIR = Path(__file__).parent.parent / "data"


# ==================== ä¸­æ–‡ NLP åœç”¨è¯è¡¨ ====================
STOPWORDS = set("""
çš„ äº† åœ¨ æ˜¯ æˆ‘ æœ‰ å’Œ å°± ä¸ äºº éƒ½ ä¸€ ä¸€ä¸ª ä¸Š ä¹Ÿ å¾ˆ åˆ° è¯´ è¦ å» ä½ 
ä¼š ç€ æ²¡æœ‰ çœ‹ å¥½ è‡ªå·± è¿™ ä»– å¥¹ å®ƒ ä»¬ é‚£ äº› ä»€ä¹ˆ äº èƒ½ å— åˆ ä¸
æŠŠ ä» å…¶ æ¯” åª ä¹‹ å¯¹ ä¸º é€šè¿‡ è€Œ å¯ä»¥ è¢« å¼€å§‹ ä»¥ å·² ä½† æ‰€ è®© æ›´
å°† åº” è¯¥ è¡Œ å‘ ä¸‹ ç„¶ å¹´æœˆæ—¥ æ—¶ ä¸­ è¿˜ é‡Œ å æ²¡ æœ€ ç¬¬ å¦‚ å›  ä¸æ˜¯
ç­‰ å°±æ˜¯ å‘¢ å§ èƒ½å¤Ÿ æ€ä¹ˆ ä¸ºä»€ä¹ˆ æ€æ · è¿™æ · é‚£æ · è¿™ä¸ª é‚£ä¸ª å¯èƒ½
åŒ…æ‹¬ æˆä¸º å› ä¸º æ‰€ä»¥ è™½ç„¶ ä½†æ˜¯ ç„¶å æˆ–è€… è€Œä¸” å› æ­¤ å¦åˆ™ å¦å¤–
åŒæ—¶ ç„¶è€Œ æ­¤å¤– ä»¥åŠ ç›¸å…³ å…³äº å·²ç» æ­£åœ¨ å¯ä»¥ éœ€è¦ è¿›è¡Œ æˆ–
æ¥è‡ª ä¹‹é—´ å…¶ä¸­ æ–¹é¢ é€šè¿‡ è¿‡ç¨‹ ç»“æ„ åœ°åŒº é—®é¢˜ å·¥ä½œ éƒ¨åˆ†
åŸæ¥ ç›®å‰ ä»Šå¤© æ˜¨å¤© æ˜å¤© ä»Šå¹´ å»å¹´ ä»Šæ—¥ è®°è€… æŠ¥é“ æ®æ‚‰
""".split())

# é¢å¤–åœç”¨è¯ï¼ˆç¤¾äº¤å¹³å°å¸¸è§å™ªéŸ³è¯ï¼‰
SOCIAL_STOPWORDS = set("""
å“ˆå“ˆ å“ˆå“ˆå“ˆ hhh ç¬‘æ­» ç»äº† å¤ªå¥½äº† çœŸçš„ å¥½çš„ ä¸é”™ æ±‚ æƒ³
èµ é¡¶ æ²™å‘ å‰æ’ æ”¶è— è½¬å‘ å…³æ³¨ ç‚¹èµ è¯„è®º åˆ†äº« é“¾æ¥
è§†é¢‘ å›¾ç‰‡ ç›´æ’­ å‘å¸ƒ æ›´æ–° æ¨è çƒ­é—¨ çƒ­æœ æœ€æ–° é€Ÿçœ‹ éœ‡æƒŠ
""".split())

# åˆ†ç±»æ ‡ç­¾è¯ï¼ˆä¸æ˜¯å®ä½“çƒ­ç‚¹ï¼Œåº”æ’é™¤ï¼‰
CATEGORY_STOPWORDS = set("""
æ—¶äº‹ è´¢ç» å›½é™… ç§‘æŠ€ æ”¿æ²» æ”¿ç» ç¤¾ä¼š å¨±ä¹ ä½“è‚² å†›äº‹ æ•™è‚² æ–‡åŒ–
æ–°é—» å¿«è®¯ å¤´æ¡ èµ„è®¯ çƒ­ç‚¹ æ¶ˆæ¯ äº‹ä»¶ ç®€è®¯ è¦é—» æ—©æŠ¥ æ™šæŠ¥
""".split())

# è‹±æ–‡åœç”¨è¯
ENGLISH_STOPWORDS = set("""
the a an is are was were be been being have has had do does did
will would shall should can could may might must need dare
to of in for on with at by from as into through during before
after above below between out off over under again further then
once here there when where why how all each every both few more
most other some such no nor not only own same so than too very
and but or if while because until although since about
it its he his she her they them their we our you your
this that these those what which who whom whose
how much many more most just also back even still
said says new report year years month day time people
""".split())

ALL_STOPWORDS = STOPWORDS | SOCIAL_STOPWORDS | CATEGORY_STOPWORDS | ENGLISH_STOPWORDS


# ==================== æ•°æ®ç»“æ„ ====================
@dataclass
class TrendTopic:
    """ä¸€ä¸ªå‘ç°çš„çƒ­ç‚¹è¯é¢˜"""
    keyword: str                    # æ ¸å¿ƒå…³é”®è¯
    heat_score: float = 0.0         # ç»¼åˆçƒ­åŠ›å€¼
    frequency: int = 0              # å½“å‰çª—å£è¯é¢‘
    acceleration: float = 0.0       # é¢‘ç‡å˜åŒ–ç‡
    source_diversity: int = 0       # å‡ºç°çš„å¹³å°æ•°
    engagement: float = 0.0         # äº’åŠ¨é‡å½’ä¸€åŒ–
    is_burst: bool = False          # æ˜¯å¦çªå‘
    burst_z_score: float = 0.0      # Z-Score
    macd_signal: str = 'neutral'    # MACDä¿¡å·: bullish/bearish/neutral
    macd_value: float = 0.0         # MACDå€¼
    trend_direction: str = 'â†’'      # è¶‹åŠ¿æ–¹å‘: â†‘â†—â†’â†˜â†“
    platforms: List[str] = field(default_factory=list)  # å‡ºç°çš„å¹³å°
    related_titles: List[str] = field(default_factory=list)  # ç›¸å…³åŸå§‹æ ‡é¢˜
    category: str = ''              # åˆ†ç±»
    sparkline: List[float] = field(default_factory=list)  # æœ€è¿‘Nä¸ªçª—å£çš„çƒ­åº¦
    first_seen: str = ''            # é¦–æ¬¡å‡ºç°æ—¶é—´
    peak_time: str = ''             # å³°å€¼æ—¶é—´
    
    def to_dict(self):
        return asdict(self)


# ==================== NLP æ–‡æœ¬å¤„ç†å™¨ ====================
class ChineseNLP:
    """
    ä¸­æ–‡ NLP å¤„ç†æµæ°´çº¿
    
    æµç¨‹ï¼šRaw Text â†’ æ¸…æ´— â†’ åˆ†è¯ â†’ è¿‡æ»¤ â†’ å…³é”®è¯æå–
    
    æŠ€æœ¯æ ˆï¼š
    - jieba åˆ†è¯ï¼ˆæ”¯æŒè‡ªå®šä¹‰è¯å…¸ï¼‰
    - jieba.analyse (TF-IDF + TextRank)
    - è‡ªå®šä¹‰åœç”¨è¯è¿‡æ»¤
    - æ–°è¯å‘ç°ï¼ˆåŸºäºäº’ä¿¡æ¯å’Œå·¦å³ç†µï¼‰
    """
    
    def __init__(self):
        self._init_jieba()

    def _init_jieba(self):
        """åˆå§‹åŒ– jieba åˆ†è¯å™¨"""
        try:
            import jieba
            import jieba.analyse
            self.jieba = jieba
            self.analyse = jieba.analyse
            
            # æ·»åŠ é¢†åŸŸä¸“æœ‰è¯æ±‡ï¼ˆé¿å…è¢«é”™è¯¯åˆ‡åˆ†ï¼‰
            custom_words = [
                'Aè‚¡', 'æ¸¯è‚¡', 'ç¾è‚¡', 'æ¯”ç‰¹å¸', 'æ•°å­—è´§å¸', 'åŒºå—é“¾', 'å…ƒå®‡å®™',
                'äººå·¥æ™ºèƒ½', 'å¤§æ¨¡å‹', 'è‡ªåŠ¨é©¾é©¶', 'é‡å­è®¡ç®—', 'åŠå¯¼ä½“', 'èŠ¯ç‰‡',
                'ç‰¹æœ—æ™®', 'æ‹œç™»', 'æ™®äº¬', 'ä¹ è¿‘å¹³', 'é©¬æ–¯å…‹',
                'ChatGPT', 'DeepSeek', 'OpenAI', 'Kimi', 'GPT4',
                'ä¸€å¸¦ä¸€è·¯', 'ä¸­ç¾å…³ç³»', 'å°æµ·', 'å—æµ·', 'åŒ—çº¦',
                'æ–°èƒ½æº', 'å…‰ä¼', 'é”‚ç”µæ± ', 'æ–°è´¨ç”Ÿäº§åŠ›',
                'é™æ¯', 'åŠ æ¯', 'å¤®è¡Œ', 'ç¾è”å‚¨', 'GDP', 'CPI', 'PMI',
                'å†…å·', 'èººå¹³', 'è€ƒå…¬', 'è€ƒç ”', 'å°±ä¸šç‡',
            ]
            for word in custom_words:
                jieba.add_word(word, freq=10000)
            
            logger.info("  âœ… jieba åˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆ")
        except ImportError:
            logger.warning("  âš ï¸ jieba æœªå®‰è£…ï¼Œå°†è‡ªåŠ¨å®‰è£…...")
            import subprocess, sys
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'jieba', '-q'])
            import jieba
            import jieba.analyse
            self.jieba = jieba
            self.analyse = jieba.analyse

    def clean_text(self, text: str) -> str:
        """
        æ–‡æœ¬æ¸…æ´—
        
        å¤„ç†ï¼š
        1. å»é™¤HTMLæ ‡ç­¾
        2. å»é™¤URL
        3. å»é™¤emoji (ä¿ç•™ä¸­è‹±æ–‡å’Œæ•°å­—)
        4. å»é™¤@æåŠ
        5. å»é™¤#è¯é¢˜æ ‡è®°
        6. å‹ç¼©ç©ºç™½
        """
        if not text:
            return ''
        
        # å»HTML
        text = re.sub(r'<[^>]+>', '', text)
        # å»URL
        text = re.sub(r'https?://\S+', '', text)
        # å»@æåŠ
        text = re.sub(r'@\w+', '', text)
        # å»HTMLå®ä½“
        text = re.sub(r'&\w+;', ' ', text)
        # ä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€å¸¸è§æ ‡ç‚¹
        text = re.sub(r'[^\u4e00-\u9fff\u3000-\u303fA-Za-z0-9\sï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹ã€ã€‘Â·%â€°â„ƒ]', ' ', text)
        # å‹ç¼©ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def tokenize(self, text: str, min_len: int = 2) -> List[str]:
        """
        åˆ†è¯ + è¿‡æ»¤
        
        ä½¿ç”¨ jieba ç²¾ç¡®æ¨¡å¼åˆ†è¯ï¼Œå»é™¤åœç”¨è¯å’ŒçŸ­è¯ã€‚
        """
        text = self.clean_text(text)
        if not text:
            return []
        
        words = self.jieba.cut(text, cut_all=False)
        result = []
        for word in words:
            word = word.strip()
            if len(word) < min_len:
                continue
            if word.lower() in ALL_STOPWORDS:
                continue
            if re.match(r'^[\d\s]+$', word):  # çº¯æ•°å­—
                continue
            result.append(word)
        
        return result

    def extract_keywords_tfidf(self, text: str, topK: int = 20) -> List[Tuple[str, float]]:
        """
        TF-IDF å…³é”®è¯æå–
        
        ä½¿ç”¨ jieba.analyse.extract_tagsï¼Œè¿”å› (å…³é”®è¯, æƒé‡) åˆ—è¡¨ã€‚
        TF-IDF é€‚åˆæå–åœ¨å½“å‰æ–‡æ¡£ä¸­é‡è¦ä½†åœ¨è¯­æ–™åº“ä¸­ä¸å¸¸è§çš„è¯ã€‚
        """
        text = self.clean_text(text)
        if not text:
            return []
        
        tags = self.analyse.extract_tags(text, topK=topK, withWeight=True)
        # è¿‡æ»¤åœç”¨è¯
        return [(word, weight) for word, weight in tags 
                if word not in ALL_STOPWORDS and len(word) >= 2]

    def extract_keywords_textrank(self, text: str, topK: int = 20) -> List[Tuple[str, float]]:
        """
        TextRank å…³é”®è¯æå–
        
        åŸºäºå›¾çš„æ’åºç®—æ³•ï¼ˆç±»ä¼¼ PageRankï¼‰ï¼Œ
        è€ƒè™‘è¯ä¸è¯ä¹‹é—´çš„å…±ç°å…³ç³»ï¼Œé€‚åˆæå–æ ¸å¿ƒæ¦‚å¿µã€‚
        """
        text = self.clean_text(text)
        if not text:
            return []
        
        tags = self.analyse.textrank(text, topK=topK, withWeight=True)
        return [(word, weight) for word, weight in tags 
                if word not in ALL_STOPWORDS and len(word) >= 2]

    def batch_extract_keywords(self, texts: List[str], topK: int = 50) -> List[Tuple[str, float]]:
        """
        æ‰¹é‡æ–‡æœ¬å…³é”®è¯æå–
        
        åˆå¹¶å¤šç¯‡æ–‡æœ¬ï¼ŒåŒæ—¶ä½¿ç”¨ TF-IDF å’Œ TextRankï¼Œ
        å–ä¸¤ç§æ–¹æ³•çš„äº¤é›†ä½œä¸ºé«˜ç½®ä¿¡åº¦å…³é”®è¯ã€‚
        """
        if not texts:
            return []
        
        combined = ' '.join(self.clean_text(t) for t in texts if t)
        
        tfidf_kws = dict(self.extract_keywords_tfidf(combined, topK=topK * 2))
        textrank_kws = dict(self.extract_keywords_textrank(combined, topK=topK * 2))
        
        # èåˆä¸¤ç§æ–¹æ³•çš„å¾—åˆ†
        all_words = set(tfidf_kws.keys()) | set(textrank_kws.keys())
        scores = {}
        for word in all_words:
            tf_score = tfidf_kws.get(word, 0)
            tr_score = textrank_kws.get(word, 0)
            # ä¸¤ç§æ–¹æ³•éƒ½å‡ºç°çš„è¯å¾—åˆ†æ›´é«˜
            if word in tfidf_kws and word in textrank_kws:
                scores[word] = (tf_score + tr_score) * 1.5
            else:
                scores[word] = tf_score + tr_score
        
        sorted_kws = sorted(scores.items(), key=lambda x: -x[1])
        return sorted_kws[:topK]

    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """
        ç®€æ˜“å®ä½“è¯†åˆ«ï¼ˆåŸºäºè§„åˆ™ + è¯å…¸ï¼‰
        
        è¯†åˆ«ï¼šäººåã€åœ°åã€ç»„ç»‡åã€å“ç‰Œ
        æ³¨ï¼šå®Œæ•´ NER å»ºè®®ä½¿ç”¨ HanLP æˆ– LACï¼Œè¿™é‡Œç”¨è½»é‡è§„åˆ™æ–¹æ¡ˆ
        """
        entities = {
            'person': [],
            'location': [],
            'organization': [],
            'brand': [],
        }
        
        text = self.clean_text(text)
        
        # äººåè¯å…¸
        person_dict = {'ä¹ è¿‘å¹³', 'ç‰¹æœ—æ™®', 'æ‹œç™»', 'æ™®äº¬', 'é©¬æ–¯å…‹', 'é©¬å…‹é¾™', 
                       'å²¸ç”°', 'æ³½è¿æ–¯åŸº', 'è«è¿ª', 'æå¼º', 'ç‹æ¯…', 'å¸ƒæ—è‚¯',
                       'æ¯”å°”ç›–èŒ¨', 'æ‰å…‹ä¼¯æ ¼', 'é»„ä»å‹‹', 'ä»»æ­£é', 'é©¬äº‘'}
        
        # åœ°åè¯å…¸
        location_dict = {'åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'å¹¿å·', 'æ­å·', 'æˆéƒ½', 'æ­¦æ±‰',
                         'ç¾å›½', 'ä¸­å›½', 'æ—¥æœ¬', 'éŸ©å›½', 'ä¿„ç½—æ–¯', 'æ¬§æ´²', 'å°æ¹¾',
                         'åç››é¡¿', 'çº½çº¦', 'ä¼¦æ•¦', 'ä¸œäº¬', 'è«æ–¯ç§‘', 'å·´é»',
                         'åŠ æ²™', 'ä»¥è‰²åˆ—', 'ä¹Œå…‹å…°', 'å™åˆ©äºš', 'ä¼Šæœ—', 'æœé²œ'}
        
        # ç»„ç»‡è¯å…¸
        org_dict = {'å¤®è¡Œ', 'ç¾è”å‚¨', 'æ¬§å¤®è¡Œ', 'å›½åŠ¡é™¢', 'å‘æ”¹å§”', 'å¤–äº¤éƒ¨',
                    'è”åˆå›½', 'åŒ—çº¦', 'æ¬§ç›Ÿ', 'ä¸–å«ç»„ç»‡', 'ä¸–è´¸ç»„ç»‡', 'äºšæŠ•è¡Œ',
                    'äººå¤§', 'æ”¿å', 'æœ€é«˜æ³•', 'æœ€é«˜æ£€'}
        
        # å“ç‰Œè¯å…¸
        brand_dict = {'åä¸º', 'è‹¹æœ', 'ç‰¹æ–¯æ‹‰', 'å°ç±³', 'è…¾è®¯', 'é˜¿é‡Œ', 'å­—èŠ‚è·³åŠ¨',
                      'ç™¾åº¦', 'OpenAI', 'è°·æ­Œ', 'å¾®è½¯', 'è‹±ä¼Ÿè¾¾', 'å°ç§¯ç”µ',
                      'æ¯”äºšè¿ª', 'å®å¾·æ—¶ä»£', 'ä¸­èŠ¯å›½é™…', 'ç†æƒ³', 'è”šæ¥', 'å°é¹'}
        
        words = set(self.tokenize(text, min_len=2))
        
        entities['person'] = list(words & person_dict)
        entities['location'] = list(words & location_dict)
        entities['organization'] = list(words & org_dict)
        entities['brand'] = list(words & brand_dict)
        
        return entities

    def discover_new_words(self, texts: List[str], min_freq: int = 3, 
                           max_len: int = 6) -> List[Tuple[str, int]]:
        """
        æ–°è¯å‘ç°
        
        åŸºäº n-gram é¢‘ç‡ç»Ÿè®¡ + äº’ä¿¡æ¯(PMI) çš„æ–°è¯å‘ç°ã€‚
        è¯†åˆ«åœ¨å¸¸è§„è¯å…¸ä¸­ä¸å­˜åœ¨ä½†é¢‘ç¹å‡ºç°çš„æ–°è¯ã€‚
        
        é€‚ç”¨åœºæ™¯ï¼šå‘ç°ç½‘ç»œçƒ­æ¢—ã€æ–°äº‹ä»¶åç§°ã€æ–°äº§å“åç­‰ã€‚
        """
        # ç»Ÿè®¡ 2-gram åˆ° max_len-gram
        ngram_freq = Counter()
        char_freq = Counter()
        
        for text in texts:
            text = self.clean_text(text)
            chars = [c for c in text if '\u4e00' <= c <= '\u9fff']
            
            for c in chars:
                char_freq[c] += 1
            
            for n in range(2, max_len + 1):
                for i in range(len(chars) - n + 1):
                    gram = ''.join(chars[i:i+n])
                    ngram_freq[gram] += 1
        
        total_chars = sum(char_freq.values()) or 1
        
        # è¿‡æ»¤ä½é¢‘ + è®¡ç®— PMI
        new_words = []
        for gram, freq in ngram_freq.items():
            if freq < min_freq:
                continue
            
            # æ£€æŸ¥æ˜¯å¦å·²åœ¨è¯å…¸ä¸­
            seg_result = list(self.jieba.cut(gram, cut_all=False))
            if len(seg_result) == 1 and seg_result[0] == gram:
                continue  # å·²ç»æ˜¯è¯å…¸ä¸­çš„è¯
            
            # ç®€åŒ– PMI: ç”¨å„å­—ç¬¦ç‹¬ç«‹é¢‘ç‡çš„ä¹˜ç§¯ vs è”åˆé¢‘ç‡
            char_probs = 1.0
            for c in gram:
                char_probs *= (char_freq.get(c, 1) / total_chars)
            
            joint_prob = freq / total_chars
            pmi = math.log(joint_prob / char_probs + 1e-10) if char_probs > 0 else 0
            
            if pmi > 2.0:  # PMI é˜ˆå€¼
                new_words.append((gram, freq))
        
        new_words.sort(key=lambda x: -x[1])
        return new_words[:50]


# ==================== æ—¶é—´åºåˆ—ç®¡ç† ====================
class TimeSeriesStore:
    """
    å…³é”®è¯æ—¶é—´åºåˆ—å­˜å‚¨
    
    ç»´æŠ¤æ¯ä¸ªå…³é”®è¯åœ¨æ¯ä¸ªæ—¶é—´çª—å£çš„å‡ºç°é¢‘ç‡ï¼Œ
    ç”¨äºçªå‘æ£€æµ‹å’Œè¶‹åŠ¿è®¡ç®—ã€‚
    
    å­˜å‚¨ç»“æ„:
    {
        "keyword": {
            "windows": [{"time": "...", "count": N, "platforms": [...], "engagement": F}],
            "first_seen": "...",
            "peak_count": N,
            "peak_time": "..."
        }
    }
    """
    
    def __init__(self, store_path: Path = None):
        self.store_path = store_path or (DATA_DIR / "keyword_history.json")
        self.data: Dict[str, Dict] = {}
        self._load()

    def _load(self):
        """åŠ è½½å†å²æ•°æ®"""
        if self.store_path.exists():
            try:
                with open(self.store_path, 'r', encoding='utf-8') as f:
                    self.data = json.load(f)
                logger.info(f"  ğŸ“‚ åŠ è½½å†å²æ•°æ®: {len(self.data)} ä¸ªå…³é”®è¯")
            except (json.JSONDecodeError, IOError):
                self.data = {}

    def save(self):
        """ä¿å­˜å†å²æ•°æ®"""
        self.store_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.store_path, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False)
        logger.info(f"  ğŸ’¾ ä¿å­˜å†å²æ•°æ®: {len(self.data)} ä¸ªå…³é”®è¯")

    def record(self, keyword: str, count: int, platforms: List[str], 
               engagement: float = 0, window_time: str = None):
        """
        è®°å½•ä¸€ä¸ªå…³é”®è¯åœ¨å½“å‰çª—å£çš„æ•°æ®
        """
        now = window_time or datetime.now(timezone.utc).isoformat()
        
        if keyword not in self.data:
            self.data[keyword] = {
                'windows': [],
                'first_seen': now,
                'peak_count': 0,
                'peak_time': now,
            }
        
        rec = self.data[keyword]
        rec['windows'].append({
            'time': now,
            'count': count,
            'platforms': platforms,
            'engagement': engagement,
        })
        
        # æ›´æ–°å³°å€¼
        if count > rec.get('peak_count', 0):
            rec['peak_count'] = count
            rec['peak_time'] = now
        
        # åªä¿ç•™æœ€è¿‘ HISTORY_WINDOWS ä¸ªçª—å£
        if len(rec['windows']) > HISTORY_WINDOWS:
            rec['windows'] = rec['windows'][-HISTORY_WINDOWS:]

    def get_series(self, keyword: str) -> List[Dict]:
        """è·å–å…³é”®è¯çš„æ—¶é—´åºåˆ—"""
        return self.data.get(keyword, {}).get('windows', [])

    def get_counts(self, keyword: str) -> List[int]:
        """è·å–å…³é”®è¯çš„è®¡æ•°åºåˆ—"""
        return [w['count'] for w in self.get_series(keyword)]

    def cleanup(self, max_age_hours: int = 48):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        cutoff = (datetime.now(timezone.utc) - timedelta(hours=max_age_hours)).isoformat()
        to_delete = []
        
        for keyword, rec in self.data.items():
            windows = rec.get('windows', [])
            if not windows:
                to_delete.append(keyword)
                continue
            # å¦‚æœæœ€æ–°çª—å£éƒ½è¿‡æœŸäº†ï¼Œåˆ é™¤
            if windows[-1].get('time', '') < cutoff:
                to_delete.append(keyword)
        
        for kw in to_delete:
            del self.data[kw]
        
        if to_delete:
            logger.info(f"  ğŸ§¹ æ¸…ç† {len(to_delete)} ä¸ªè¿‡æœŸå…³é”®è¯")


# ==================== çªå‘æ£€æµ‹ç®—æ³• ====================
class BurstDetector:
    """
    çªå‘æ£€æµ‹å™¨
    
    é›†æˆä¸‰ç§æ£€æµ‹æ–¹æ³•ï¼š
    1. Z-Score å¼‚å¸¸æ£€æµ‹ - ç»Ÿè®¡åç¦»åº¦
    2. MACD è¶‹åŠ¿åŠ¨é‡ - è¶‹åŠ¿å¯åŠ¨/ç»“æŸåˆ¤æ–­
    3. Newton Cooling è¡°å‡ - çƒ­åº¦æ—¶é—´ä»·å€¼
    """

    @staticmethod
    def z_score_detect(counts: List[int]) -> Tuple[float, bool]:
        """
        Z-Score å¼‚å¸¸æ£€æµ‹
        
        å…¬å¼: z = (x_t - Î¼) / Ïƒ
        
        å½“å‰å€¼ç›¸å¯¹å†å²å¹³å‡å€¼çš„åç¦»ç¨‹åº¦ã€‚
        z > 2.5 è§†ä¸ºçªå‘ï¼ˆ99.4%ç½®ä¿¡åº¦ï¼‰
        z > 3.0 è§†ä¸ºå¼ºçªå‘ï¼ˆ99.7%ç½®ä¿¡åº¦ï¼‰
        
        Args:
            counts: å†å²è®¡æ•°åºåˆ—
            
        Returns:
            (z_score, is_burst)
        """
        if len(counts) < 3:
            return 0.0, False
        
        current = counts[-1]
        historical = counts[:-1]
        
        mean = sum(historical) / len(historical)
        variance = sum((x - mean) ** 2 for x in historical) / len(historical)
        std = math.sqrt(variance) if variance > 0 else 1.0
        
        z = (current - mean) / std if std > 0 else 0.0
        
        return z, z > BURST_Z_THRESHOLD

    @staticmethod
    def ema(values: List[float], period: int) -> List[float]:
        """è®¡ç®—æŒ‡æ•°ç§»åŠ¨å¹³å‡ (EMA)"""
        if not values:
            return []
        
        multiplier = 2.0 / (period + 1)
        ema_values = [values[0]]
        
        for val in values[1:]:
            ema_val = (val - ema_values[-1]) * multiplier + ema_values[-1]
            ema_values.append(ema_val)
        
        return ema_values

    @staticmethod
    def macd_detect(counts: List[int]) -> Tuple[float, str]:
        """
        MACD è¶‹åŠ¿åŠ¨é‡æ£€æµ‹
        
        å€Ÿé‰´é‡‘èæŠ€æœ¯åˆ†æçš„ MACD æŒ‡æ ‡ï¼š
        - çŸ­æœŸEMA(12) - é•¿æœŸEMA(26) = MACDçº¿
        - MACDçš„EMA(9) = ä¿¡å·çº¿
        - MACD > Signal ä¸”ä¸Šç©¿ â†’ bullish (è¶‹åŠ¿å¯åŠ¨)
        - MACD < Signal ä¸”ä¸‹ç©¿ â†’ bearish (è¶‹åŠ¿è¡°é€€)
        
        Args:
            counts: å†å²è®¡æ•°åºåˆ—
            
        Returns:
            (macd_value, signal_str) signal_str âˆˆ {'bullish', 'bearish', 'neutral'}
        """
        if len(counts) < MACD_LONG_PERIOD:
            return 0.0, 'neutral'
        
        float_counts = [float(c) for c in counts]
        
        short_ema = BurstDetector.ema(float_counts, MACD_SHORT_PERIOD)
        long_ema = BurstDetector.ema(float_counts, MACD_LONG_PERIOD)
        
        macd_line = [s - l for s, l in zip(short_ema, long_ema)]
        signal_line = BurstDetector.ema(macd_line, MACD_SIGNAL_PERIOD)
        
        if not signal_line or not macd_line:
            return 0.0, 'neutral'
        
        macd_current = macd_line[-1]
        signal_current = signal_line[-1]
        
        # åˆ¤æ–­äº¤å‰
        if len(macd_line) >= 2 and len(signal_line) >= 2:
            prev_diff = macd_line[-2] - signal_line[-2]
            curr_diff = macd_current - signal_current
            
            if prev_diff <= 0 and curr_diff > 0:
                return macd_current, 'bullish'   # é‡‘å‰
            elif prev_diff >= 0 and curr_diff < 0:
                return macd_current, 'bearish'   # æ­»å‰
        
        if macd_current > signal_current:
            return macd_current, 'bullish'
        elif macd_current < signal_current:
            return macd_current, 'bearish'
        
        return macd_current, 'neutral'

    @staticmethod
    def newton_cooling_decay(peak_value: float, hours_since_peak: float) -> float:
        """
        ç‰›é¡¿å†·å´å®šå¾‹è¡°å‡
        
        T(t) = T_peak Â· e^{-Î»t}
        
        æ¨¡æ‹Ÿçƒ­ç‚¹çš„è‡ªç„¶é™æ¸©è¿‡ç¨‹ã€‚
        åŠè¡°æœŸ = ln(2)/Î» â‰ˆ 4å°æ—¶ï¼ˆå³4å°æ—¶çƒ­åº¦é™ä¸€åŠï¼‰
        
        Args:
            peak_value: å³°å€¼çƒ­åº¦
            hours_since_peak: è·ç¦»å³°å€¼çš„å°æ—¶æ•°
            
        Returns:
            è¡°å‡åçš„çƒ­åº¦å€¼
        """
        return peak_value * math.exp(-LAMBDA_DECAY * max(0, hours_since_peak))

    @staticmethod
    def calculate_acceleration(counts: List[int]) -> float:
        """
        è®¡ç®—é¢‘ç‡åŠ é€Ÿåº¦ (dF/dt)
        
        ä½¿ç”¨æœ€è¿‘3ä¸ªçª—å£çš„äºŒé˜¶å·®åˆ†ã€‚
        æ­£å€¼ = åŠ é€Ÿå¢é•¿ï¼Œè´Ÿå€¼ = å‡é€Ÿ/ä¸‹é™ã€‚
        """
        if len(counts) < 3:
            if len(counts) == 2:
                return float(counts[-1] - counts[-2])
            return 0.0
        
        # ä¸€é˜¶å·®åˆ†
        d1 = counts[-1] - counts[-2]
        d2 = counts[-2] - counts[-3]
        
        # äºŒé˜¶å·®åˆ† (åŠ é€Ÿåº¦)
        acceleration = d1 - d2
        
        # ä¹Ÿè€ƒè™‘ä¸€é˜¶å˜åŒ–ç‡ (é€Ÿåº¦)
        velocity = d1
        
        # ç»¼åˆ: é€Ÿåº¦ + åŠ é€Ÿåº¦å„å ä¸€åŠ
        return velocity * 0.6 + acceleration * 0.4


# ==================== çƒ­åº¦è¯„åˆ†å™¨ ====================
class HeatScorer:
    """
    ç»¼åˆçƒ­åº¦è¯„åˆ†å™¨
    
    å°† NLP æå–çš„å…³é”®è¯ + æ—¶é—´åºåˆ—æ•°æ® + çªå‘æ£€æµ‹ç»“æœ
    ç»¼åˆè®¡ç®—æ¯ä¸ªå…³é”®è¯çš„å®æ—¶çƒ­åŠ›å€¼ã€‚
    """

    @staticmethod
    def compute_heat(keyword: str, freq: int, acceleration: float,
                     source_count: int, engagement: float,
                     hours_since_peak: float = 0) -> float:
        """
        è®¡ç®—å®æ—¶çƒ­åŠ›å€¼
        
        H(t) = Î±Â·F(t)Â·e^{-Î»Î”t} + Î²Â·A(t) + Î³Â·S(t) + Î´Â·E(t)
        
        Args:
            keyword: å…³é”®è¯
            freq: å½“å‰çª—å£é¢‘ç‡
            acceleration: é¢‘ç‡åŠ é€Ÿåº¦
            source_count: æ¥æºå¹³å°æ•°ï¼ˆ1-6ï¼‰
            engagement: å½’ä¸€åŒ–äº’åŠ¨é‡ï¼ˆ0-1ï¼‰
            hours_since_peak: è·ç¦»å³°å€¼æ—¶é—´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            çƒ­åŠ›å€¼ï¼ˆ0-100 æ ‡å‡†åŒ–ï¼‰
        """
        # F(t) Â· è¡°å‡å› å­
        freq_with_decay = freq * math.exp(-LAMBDA_DECAY * hours_since_peak)
        
        # æ ‡å‡†åŒ–å„åˆ†é‡åˆ°ç›¸ä¼¼é‡çº§
        f_norm = min(freq_with_decay / 10.0, 10.0)  # é¢‘ç‡æ ‡å‡†åŒ–
        a_norm = max(min(acceleration / 5.0, 5.0), -5.0)  # åŠ é€Ÿåº¦æ ‡å‡†åŒ–
        s_norm = source_count / 3.0  # æ¥æºå¤šæ ·æ€§æ ‡å‡†åŒ– (3ä¸ªå¹³å°=1.0)
        e_norm = min(engagement, 1.0)  # äº’åŠ¨é‡å·²ç»æ˜¯0-1
        
        # ç»¼åˆæ‰“åˆ†
        raw_score = (
            ALPHA * f_norm +
            BETA  * max(0, a_norm) +   # åŠ é€Ÿåº¦åªå–æ­£å€¼
            GAMMA * s_norm +
            DELTA * e_norm
        )
        
        # æ˜ å°„åˆ° 0-100
        heat = min(100.0, raw_score * 15.0)
        
        return round(heat, 2)

    @staticmethod
    def determine_direction(counts: List[int]) -> str:
        """
        åˆ¤æ–­è¶‹åŠ¿æ–¹å‘
        
        åŸºäºæœ€è¿‘çª—å£çš„å˜åŒ–ç‡:
        â†‘ å¿«é€Ÿä¸Šå‡ (>50%)
        â†— ç¼“æ…¢ä¸Šå‡ (10-50%)
        â†’ æŒå¹³ (-10% to +10%)
        â†˜ ç¼“æ…¢ä¸‹é™ (-50% to -10%)
        â†“ å¿«é€Ÿä¸‹é™ (<-50%)
        """
        if len(counts) < 2:
            return 'â†’'
        
        current = counts[-1]
        previous = counts[-2] if counts[-2] > 0 else 1
        
        change_rate = (current - previous) / previous
        
        if change_rate > 0.5:
            return 'â†‘'
        elif change_rate > 0.1:
            return 'â†—'
        elif change_rate > -0.1:
            return 'â†’'
        elif change_rate > -0.5:
            return 'â†˜'
        else:
            return 'â†“'


# ==================== è¶‹åŠ¿å‘ç°å¼•æ“ ====================
class TrendEngine:
    """
    è¶‹åŠ¿å‘ç°å¼•æ“ - ä¸²è” NLP + æ—¶é—´åºåˆ— + çªå‘æ£€æµ‹
    
    å®Œæ•´æµç¨‹ï¼š
    1. æ¥æ”¶åŸå§‹é‡‡é›†æ•°æ®
    2. NLP æå–å…³é”®è¯
    3. ç»Ÿè®¡è¯é¢‘ + æ¥æºåˆ†å¸ƒ + äº’åŠ¨é‡
    4. æ›´æ–°æ—¶é—´åºåˆ—
    5. è¿è¡Œçªå‘æ£€æµ‹
    6. è®¡ç®—ç»¼åˆçƒ­åŠ›å€¼
    7. æ’åºè¾“å‡º Top-N è¶‹åŠ¿
    """
    
    def __init__(self):
        self.nlp = ChineseNLP()
        self.ts_store = TimeSeriesStore()
        self.burst_detector = BurstDetector()
        self.heat_scorer = HeatScorer()

    def process(self, raw_contents: list, topK: int = 50) -> List[TrendTopic]:
        """
        ä¸»å¤„ç†æµç¨‹
        
        Args:
            raw_contents: RawContent å¯¹è±¡åˆ—è¡¨ï¼ˆæ¥è‡ª feed_crawlerï¼‰
            topK: è¿”å›å‰Kä¸ªè¶‹åŠ¿
            
        Returns:
            TrendTopic åˆ—è¡¨ï¼ŒæŒ‰çƒ­åŠ›å€¼é™åº
        """
        if not raw_contents:
            return []
        
        logger.info(f"\nğŸ”¬ è¶‹åŠ¿åˆ†æå¼•æ“å¯åŠ¨ [{len(raw_contents)} æ¡å†…å®¹]")
        
        # â”€â”€ Step 1: NLP å…³é”®è¯æå– â”€â”€
        logger.info("  ğŸ“ Step 1/5: NLP å…³é”®è¯æå–...")
        keyword_data = self._extract_all_keywords(raw_contents)
        logger.info(f"     â†’ æå– {len(keyword_data)} ä¸ªå…³é”®è¯")
        
        # â”€â”€ Step 2: ç»Ÿè®¡è¯é¢‘ã€æ¥æºã€äº’åŠ¨ â”€â”€
        logger.info("  ğŸ“Š Step 2/5: ç»Ÿè®¡åˆ†æ...")
        freq_stats = self._compute_frequency_stats(keyword_data, raw_contents)
        
        # â”€â”€ Step 3: æ›´æ–°æ—¶é—´åºåˆ— â”€â”€
        logger.info("  ğŸ“ˆ Step 3/5: æ›´æ–°æ—¶é—´åºåˆ—...")
        self._update_time_series(freq_stats)
        
        # â”€â”€ Step 4: çªå‘æ£€æµ‹ â”€â”€
        logger.info("  ğŸš¨ Step 4/5: çªå‘æ£€æµ‹...")
        burst_results = self._run_burst_detection(freq_stats)
        
        # â”€â”€ Step 5: è®¡ç®—çƒ­åŠ›å€¼ & æ’åº â”€â”€
        logger.info("  ğŸ”¥ Step 5/5: è®¡ç®—çƒ­åŠ›å€¼...")
        trends = self._score_and_rank(freq_stats, burst_results, topK)
        
        # ä¿å­˜æ—¶é—´åºåˆ—
        self.ts_store.save()
        self.ts_store.cleanup(max_age_hours=48)
        
        logger.info(f"\nğŸ¯ å‘ç° {len(trends)} ä¸ªè¶‹åŠ¿è¯é¢˜:")
        for i, t in enumerate(trends[:10]):
            burst_mark = 'ğŸ”´' if t.is_burst else 'âšª'
            logger.info(f"   {i+1}. [{t.heat_score:5.1f}] {burst_mark} {t.keyword} "
                        f"{t.trend_direction} ({','.join(t.platforms)})")
        
        return trends

    def _extract_all_keywords(self, raw_contents: list) -> Dict[str, Dict]:
        """
        ä»æ‰€æœ‰å†…å®¹ä¸­æå–å…³é”®è¯
        
        Returns:
            {keyword: {
                'weight': float,         # NLPæƒé‡
                'sources': set(),        # æ¥æºå¹³å°é›†åˆ
                'titles': list(),        # ç›¸å…³æ ‡é¢˜
                'engagement': float,     # äº’åŠ¨é‡
            }}
        """
        keyword_data = defaultdict(lambda: {
            'weight': 0.0,
            'sources': set(),
            'titles': [],
            'engagement': 0.0,
        })
        
        # æ”¶é›†æ‰€æœ‰æ–‡æœ¬
        all_texts = []
        for item in raw_contents:
            text = getattr(item, 'title', item.get('title', '')) if isinstance(item, dict) else item.title
            all_texts.append(text)
        
        # æ‰¹é‡æå–å…³é”®è¯
        combined_keywords = self.nlp.batch_extract_keywords(all_texts, topK=100)
        keyword_set = {kw for kw, _ in combined_keywords}
        
        # ä¸ºæ¯ä¸ªå†…å®¹é¡¹æ ‡è®°å…³é”®è¯
        for item in raw_contents:
            if isinstance(item, dict):
                title = item.get('title', '')
                platform = item.get('platform', '')
                engagement = float(item.get('likes', 0)) * 1 + float(item.get('comments', 0)) * 3
            else:
                title = item.title
                platform = item.platform
                engagement = item.engagement_score()
            
            # åˆ†è¯å¹¶åŒ¹é…å…³é”®è¯
            words = self.nlp.tokenize(title)
            for word in words:
                # å¿…é¡»åœ¨å…³é”®è¯é›†ä¸­æˆ–æ˜¯æœ‰æ„ä¹‰çš„é•¿è¯ï¼ˆæ’é™¤åœç”¨è¯ï¼‰
                if word.lower() in ALL_STOPWORDS:
                    continue
                if word in keyword_set or len(word) >= 3:
                    kd = keyword_data[word]
                    kd['weight'] += 1.0
                    kd['sources'].add(platform)
                    if len(kd['titles']) < 5:
                        kd['titles'].append(title)
                    kd['engagement'] += engagement
            
            # æå–æ ‡ç­¾
            tags = []
            if isinstance(item, dict):
                tags = item.get('tags', [])
            else:
                tags = item.tags if hasattr(item, 'tags') else []
            
            for tag in tags:
                if tag and len(tag) >= 2 and tag not in ALL_STOPWORDS:
                    kd = keyword_data[tag]
                    kd['weight'] += 2.0  # æ ‡ç­¾æƒé‡æ›´é«˜
                    kd['sources'].add(platform)
                    if len(kd['titles']) < 5:
                        kd['titles'].append(title)
                    kd['engagement'] += engagement
        
        # åˆå¹¶ NLP æƒé‡
        for kw, w in combined_keywords:
            if kw in keyword_data:
                keyword_data[kw]['weight'] += w * 10
        
        return dict(keyword_data)

    def _compute_frequency_stats(self, keyword_data: Dict, 
                                  raw_contents: list) -> Dict[str, Dict]:
        """
        è®¡ç®—è¯é¢‘ç»Ÿè®¡
        
        Returns:
            {keyword: {
                'frequency': int,
                'platforms': list,
                'engagement_norm': float,
                'weight': float,
                'titles': list,
            }}
        """
        # è®¡ç®—äº’åŠ¨é‡çš„æœ€å¤§å€¼ç”¨äºå½’ä¸€åŒ–
        max_engagement = max((kd['engagement'] for kd in keyword_data.values()), default=1.0) or 1.0
        
        stats = {}
        for keyword, kd in keyword_data.items():
            freq = int(kd['weight'])
            if freq < 2:  # è¿‡æ»¤ä½é¢‘è¯
                continue
            
            stats[keyword] = {
                'frequency': freq,
                'platforms': list(kd['sources']),
                'engagement_norm': min(kd['engagement'] / max_engagement, 1.0),
                'weight': kd['weight'],
                'titles': kd['titles'],
            }
        
        return stats

    def _update_time_series(self, freq_stats: Dict[str, Dict]):
        """æ›´æ–°æ—¶é—´åºåˆ—å­˜å‚¨"""
        now = datetime.now(timezone.utc).isoformat()
        
        for keyword, stats in freq_stats.items():
            self.ts_store.record(
                keyword=keyword,
                count=stats['frequency'],
                platforms=stats['platforms'],
                engagement=stats['engagement_norm'],
                window_time=now
            )

    def _run_burst_detection(self, freq_stats: Dict[str, Dict]) -> Dict[str, Dict]:
        """
        å¯¹æ¯ä¸ªå…³é”®è¯è¿è¡Œçªå‘æ£€æµ‹
        
        Returns:
            {keyword: {
                'z_score': float,
                'is_burst': bool,
                'macd_value': float,
                'macd_signal': str,
                'acceleration': float,
                'direction': str,
                'sparkline': list,
            }}
        """
        results = {}
        
        for keyword in freq_stats:
            counts = self.ts_store.get_counts(keyword)
            
            # Z-Score çªå‘æ£€æµ‹
            z_score, is_burst = self.burst_detector.z_score_detect(counts)
            
            # MACD è¶‹åŠ¿åŠ¨é‡
            macd_value, macd_signal = self.burst_detector.macd_detect(counts)
            
            # åŠ é€Ÿåº¦
            acceleration = self.burst_detector.calculate_acceleration(counts)
            
            # è¶‹åŠ¿æ–¹å‘
            direction = self.heat_scorer.determine_direction(counts)
            
            # è¿·ä½  sparkline (æœ€è¿‘ 20 ä¸ªçª—å£)
            sparkline = counts[-20:] if len(counts) >= 2 else counts
            
            results[keyword] = {
                'z_score': z_score,
                'is_burst': is_burst,
                'macd_value': macd_value,
                'macd_signal': macd_signal,
                'acceleration': acceleration,
                'direction': direction,
                'sparkline': sparkline,
            }
        
        burst_count = sum(1 for r in results.values() if r['is_burst'])
        bullish_count = sum(1 for r in results.values() if r['macd_signal'] == 'bullish')
        logger.info(f"     â†’ {burst_count} ä¸ªçªå‘, {bullish_count} ä¸ªä¸Šå‡è¶‹åŠ¿")
        
        return results

    def _score_and_rank(self, freq_stats: Dict, burst_results: Dict, 
                        topK: int) -> List[TrendTopic]:
        """è®¡ç®—ç»¼åˆçƒ­åŠ›å€¼å¹¶æ’åº"""
        trends = []
        
        for keyword, stats in freq_stats.items():
            burst = burst_results.get(keyword, {})
            series = self.ts_store.get_series(keyword)
            
            # è®¡ç®—è·å³°å€¼æ—¶é—´
            rec = self.ts_store.data.get(keyword, {})
            peak_time_str = rec.get('peak_time', '')
            hours_since_peak = 0
            if peak_time_str:
                try:
                    peak_dt = datetime.fromisoformat(peak_time_str.replace('Z', '+00:00'))
                    hours_since_peak = (datetime.now(timezone.utc) - peak_dt).total_seconds() / 3600
                except (ValueError, TypeError):
                    pass
            
            # è®¡ç®—çƒ­åŠ›å€¼
            heat = self.heat_scorer.compute_heat(
                keyword=keyword,
                freq=stats['frequency'],
                acceleration=burst.get('acceleration', 0),
                source_count=len(stats['platforms']),
                engagement=stats['engagement_norm'],
                hours_since_peak=hours_since_peak,
            )
            
            # çªå‘åŠ æˆ (burst çš„è¯çƒ­åº¦ Ã—1.5)
            if burst.get('is_burst', False):
                heat = min(100, heat * 1.5)
            
            # MACD bullish åŠ æˆ
            if burst.get('macd_signal', '') == 'bullish':
                heat = min(100, heat * 1.2)
            
            # åˆ†ç±»
            category = self._classify_keyword(keyword)
            
            trend = TrendTopic(
                keyword=keyword,
                heat_score=heat,
                frequency=stats['frequency'],
                acceleration=burst.get('acceleration', 0),
                source_diversity=len(stats['platforms']),
                engagement=stats['engagement_norm'],
                is_burst=burst.get('is_burst', False),
                burst_z_score=burst.get('z_score', 0),
                macd_signal=burst.get('macd_signal', 'neutral'),
                macd_value=burst.get('macd_value', 0),
                trend_direction=burst.get('direction', 'â†’'),
                platforms=stats['platforms'],
                related_titles=stats['titles'][:5],
                category=category,
                sparkline=burst.get('sparkline', []),
                first_seen=rec.get('first_seen', ''),
                peak_time=rec.get('peak_time', ''),
            )
            trends.append(trend)
        
        # æŒ‰çƒ­åŠ›å€¼é™åºæ’åº
        trends.sort(key=lambda t: -t.heat_score)
        
        return trends[:topK]

    def _classify_keyword(self, keyword: str) -> str:
        """å…³é”®è¯åˆ†ç±»"""
        text = keyword
        
        finance_kw = {'è‚¡', 'åŸºé‡‘', 'ç†è´¢', 'æŠ•èµ„', 'è´¢ç»', 'ä¸Šå¸‚', 'æ¶¨åœ', 'è·Œåœ', 
                      'Aè‚¡', 'æ¸¯è‚¡', 'ç¾è‚¡', 'å¤®è¡Œ', 'åˆ©ç‡', 'GDP', 'ç»æµ', 'é‡‘è',
                      'é“¶è¡Œ', 'ä¿é™©', 'æœŸè´§', 'æ¯”ç‰¹å¸', 'æ•°å­—è´§å¸', 'é»„é‡‘', 'çŸ³æ²¹',
                      'æˆ¿ä»·', 'æ¥¼å¸‚', 'æ¶ˆè´¹', 'å‡ºå£', 'è¿›å£', 'è´¸æ˜“'}
        politics_kw = {'æ”¿æ²»', 'æ”¿åºœ', 'æ”¿ç­–', 'å¤–äº¤', 'åˆ¶è£', 'é€‰ä¸¾', 'å†›äº‹', 'å›½é˜²',
                       'æ€»ç»Ÿ', 'é¢†å¯¼', 'æ”¹é©', 'æ³•æ¡ˆ', 'æ¡çº¦', 'ä¸¤ä¼š'}
        tech_kw = {'AI', 'äººå·¥æ™ºèƒ½', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'å¤§æ¨¡å‹', 'æœºå™¨äºº', 'ç§‘æŠ€',
                   'äº’è”ç½‘', 'æ‰‹æœº', 'åä¸º', 'è‹¹æœ', 'æ–°èƒ½æº', 'è‡ªåŠ¨é©¾é©¶', 'é‡å­'}
        intl_kw = {'ç¾å›½', 'ä¿„ç½—æ–¯', 'æ—¥æœ¬', 'éŸ©å›½', 'æ¬§æ´²', 'ä¸­ä¸œ', 'ä»¥è‰²åˆ—',
                   'ä¹Œå…‹å…°', 'åŒ—çº¦', 'è”åˆå›½', 'å›½é™…', 'å…¨çƒ'}
        
        for kw in finance_kw:
            if kw in text:
                return 'è´¢ç»'
        for kw in politics_kw:
            if kw in text:
                return 'æ”¿æ²»'
        for kw in tech_kw:
            if kw in text:
                return 'ç§‘æŠ€'
        for kw in intl_kw:
            if kw in text:
                return 'å›½é™…'
        return 'æ—¶äº‹'

    def save_trends(self, trends: List[TrendTopic], 
                    output_path: Path = None) -> Path:
        """ä¿å­˜è¶‹åŠ¿åˆ†æç»“æœ"""
        path = output_path or (DATA_DIR / "trends.json")
        path.parent.mkdir(parents=True, exist_ok=True)
        
        output = {
            'update_time': datetime.now(timezone.utc).isoformat(),
            'total_trends': len(trends),
            'burst_count': sum(1 for t in trends if t.is_burst),
            'algorithm': {
                'heat_weights': {'alpha': ALPHA, 'beta': BETA, 'gamma': GAMMA, 'delta': DELTA},
                'decay_half_life_hours': HALF_LIFE_HOURS,
                'burst_z_threshold': BURST_Z_THRESHOLD,
                'macd_periods': {'short': MACD_SHORT_PERIOD, 'long': MACD_LONG_PERIOD, 'signal': MACD_SIGNAL_PERIOD},
            },
            'trends': [t.to_dict() for t in trends],
        }
        
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        logger.info(f"  ğŸ’¾ è¶‹åŠ¿ç»“æœä¿å­˜: {path}")
        return path


# ================================
# æ¨¡å—å¯¼å‡º
# ================================
__all__ = [
    'TrendTopic', 'ChineseNLP', 'TimeSeriesStore',
    'BurstDetector', 'HeatScorer', 'TrendEngine',
]

if __name__ == '__main__':
    # ç‹¬ç«‹æµ‹è¯•ï¼šNLP å…³é”®è¯æå–ç¤ºä¾‹
    logging.basicConfig(level=logging.INFO, format='%(message)s')
    
    nlp = ChineseNLP()
    
    test_texts = [
        "å¤®è¡Œå®£å¸ƒé™æ¯25ä¸ªåŸºç‚¹ï¼ŒAè‚¡å¸‚åœºå…¨çº¿é«˜å¼€",
        "åä¸ºå‘å¸ƒæœ€æ–°è‡ªç ”èŠ¯ç‰‡ï¼Œçªç ´ç¾å›½åˆ¶è£å°é”",
        "ç‰¹æ–¯æ‹‰ä¸Šæµ·è¶…çº§å·¥å‚äº§èƒ½å†åˆ›æ–°é«˜",
        "DeepSeekå‘å¸ƒæ–°ä¸€ä»£å¤§æ¨¡å‹ï¼Œæ€§èƒ½è¶…è¶ŠGPT-4",
        "ä¸­ç¾å…³ç³»ç´§å¼ ï¼Œå¤–äº¤éƒ¨å›åº”åˆ¶è£æªæ–½",
        "æ¯”ç‰¹å¸çªç ´10ä¸‡ç¾å…ƒï¼ŒåŠ å¯†è´§å¸å¸‚åœºç‹‚æ¬¢",
        "æ˜¥èŠ‚æ¡£ç”µå½±ç¥¨æˆ¿çªç ´100äº¿ï¼Œåˆ·æ–°å†å²çºªå½•",
        "æ–°èƒ½æºæ±½è½¦é”€é‡é¦–æ¬¡è¶…è¿‡ç‡ƒæ²¹è½¦",
    ]
    
    print("\n" + "="*60)
    print("ğŸ“ NLP å…³é”®è¯æå–æµ‹è¯•")
    print("="*60)
    
    for text in test_texts:
        kws = nlp.extract_keywords_tfidf(text, topK=5)
        print(f"\nğŸ“° {text}")
        print(f"   å…³é”®è¯: {', '.join(f'{w}({s:.2f})' for w, s in kws)}")
    
    print("\n" + "="*60)
    print("ğŸ“Š æ‰¹é‡å…³é”®è¯æå–")
    print("="*60)
    
    batch_kws = nlp.batch_extract_keywords(test_texts, topK=20)
    for i, (word, score) in enumerate(batch_kws):
        print(f"   {i+1:2d}. {word:10s} â†’ {score:.4f}")
    
    # æ–°è¯å‘ç°æµ‹è¯•
    print("\n" + "="*60)
    print("ğŸ†• æ–°è¯å‘ç°")
    print("="*60)
    
    new_words = nlp.discover_new_words(test_texts * 3, min_freq=2)
    for word, freq in new_words[:10]:
        print(f"   {word} (å‡ºç° {freq} æ¬¡)")
    
    # çªå‘æ£€æµ‹æµ‹è¯•
    print("\n" + "="*60)
    print("ğŸš¨ çªå‘æ£€æµ‹æµ‹è¯•")
    print("="*60)
    
    # æ¨¡æ‹Ÿæ—¶é—´åºåˆ—ï¼šå‰é¢å¹³ç¨³ï¼Œæœ€åçªå¢
    normal_counts =  [5, 6, 4, 7, 5, 6, 5, 4, 6, 5, 5, 7, 6, 5]
    burst_counts  =  [5, 6, 4, 7, 5, 6, 5, 4, 6, 5, 5, 7, 6, 25]  # æœ€åä¸€ä¸ªçªå¢
    
    z1, is_burst1 = BurstDetector.z_score_detect(normal_counts)
    z2, is_burst2 = BurstDetector.z_score_detect(burst_counts)
    
    print(f"   æ­£å¸¸åºåˆ—: z={z1:.2f}, burst={is_burst1}")
    print(f"   çªå‘åºåˆ—: z={z2:.2f}, burst={is_burst2}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ!")
